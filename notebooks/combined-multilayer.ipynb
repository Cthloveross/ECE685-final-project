{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5539721",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for google.colab:colab:f95a1220-3100-47b6-8ef5-8a2200a15246"
     ]
    }
   ],
   "source": [
    "# üéì ECE685 Project 2: Multi-Layer SAE-Guided LLM Safety\n",
    "# ============================================================================\n",
    "# \n",
    "# This notebook extends the single-layer approach by hooking SAEs to MULTIPLE \n",
    "# layers of the LLM (layers 12 and 20), concatenating the features, and \n",
    "# comparing performance with the single-layer baseline.\n",
    "#\n",
    "# Key improvements:\n",
    "# 1. Hook SAE at Layer 12 (mid-level features) AND Layer 20 (high-level features)\n",
    "# 2. Concatenate features from both layers for richer representations\n",
    "# 3. Compare single-layer vs multi-layer detection performance\n",
    "# 4. Test multi-layer steering\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üéì ECE685 Project 2: Multi-Layer SAE Experiment\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This notebook hooks SAEs at MULTIPLE layers to boost performance.\")\n",
    "print(\"Layers: 12 (mid-level) + 20 (high-level semantic)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a10ad69",
   "metadata": {},
   "source": [
    "# üéì ECE685 Project 2: Multi-Layer SAE-Guided LLM Safety\n",
    "\n",
    "## Exploring Sparsity Across Multiple Layers\n",
    "\n",
    "This notebook extends our single-layer approach by **hooking SAEs to multiple layers** of the LLM to boost performance.\n",
    "\n",
    "### Key Hypothesis\n",
    "- **Layer 12** (middle): Captures structural and syntactic features\n",
    "- **Layer 20** (deep): Captures more abstract semantic concepts\n",
    "- **Combined**: Concatenating features from both layers provides richer representations\n",
    "\n",
    "### What We'll Do:\n",
    "1. **Load models**: Gemma-2-2B-IT + SAEs from Gemma Scope for layers 12 AND 20\n",
    "2. **Multi-layer capture**: Hook both layers, encode with respective SAEs\n",
    "3. **Feature comparison**: Compare single-layer vs multi-layer feature discovery\n",
    "4. **Detection boost**: Train classifiers on concatenated features\n",
    "5. **Steering comparison**: Test if multi-layer steering improves results\n",
    "\n",
    "### Experimental Design:\n",
    "| Condition | Layers | Feature Dim | Expected Benefit |\n",
    "|-----------|--------|-------------|------------------|\n",
    "| Single-L12 | 12 only | 16,384 | Baseline (structural) |\n",
    "| Single-L20 | 20 only | 16,384 | High-level semantic |\n",
    "| Multi-Layer | 12 + 20 | 32,768 | Combined strengths |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6352c633",
   "metadata": {},
   "source": [
    "## üîß Setup: Install Dependencies & Login to HuggingFace\n",
    "\n",
    "### ‚ö†Ô∏è IMPORTANT: Follow These Steps Exactly!\n",
    "\n",
    "| Step | Action |\n",
    "|------|--------|\n",
    "| 1Ô∏è‚É£ | **Run the installation cell below** (wait for it to finish, ~2-3 min) |\n",
    "| 2Ô∏è‚É£ | **RESTART THE RUNTIME**: Click `Runtime ‚Üí Restart runtime` |\n",
    "| 3Ô∏è‚É£ | **After restart, SKIP the installation cell** (don't run it again!) |\n",
    "| 4Ô∏è‚É£ | **Run the verification cell** to confirm everything works |\n",
    "| 5Ô∏è‚É£ | Continue with the HuggingFace login and rest of notebook |\n",
    "\n",
    "> üõë **Why restart?** Google Colab caches numpy in memory. After installing a new version, you MUST restart for Python to use the new version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014aa901",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for google.colab:colab:f95a1220-3100-47b6-8ef5-8a2200a15246"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Install dependencies (RUN THIS FIRST, THEN RESTART!)\n",
    "# ============================================================================\n",
    "# \n",
    "# ‚ö†Ô∏è  CRITICAL: After this cell finishes, you MUST:\n",
    "#     1. Click Runtime ‚Üí Restart runtime (or Ctrl+M .)\n",
    "#     2. After restart, SKIP this cell\n",
    "#     3. Run the verification cell below\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "# Uninstall any existing numpy first to avoid conflicts\n",
    "%pip uninstall -y numpy\n",
    "# Install compatible numpy version\n",
    "%pip install numpy==1.26.4\n",
    "# Install other dependencies\n",
    "%pip install -q transformers accelerate datasets torch pandas matplotlib scikit-learn tqdm\n",
    "# Install sae-lens (must be after numpy to avoid conflicts)\n",
    "%pip install -q sae-lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1550f3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.0.2\n",
      "\n",
      "======================================================================\n",
      "‚ùå ERROR: Wrong NumPy version detected!\n",
      "======================================================================\n",
      "\n",
      "   You have NumPy 2.0.2, but need 1.26.x\n",
      "\n",
      "   FIX: Click Runtime ‚Üí Restart runtime, then run THIS cell again\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Please restart the runtime and try again!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-237723205.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n   FIX: Click Runtime ‚Üí Restart runtime, then run THIS cell again\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please restart the runtime and try again!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚úì NumPy version: {np.__version__} (compatible)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Please restart the runtime and try again!"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Verify installation (run AFTER restarting runtime)\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "if np.__version__.startswith(\"2.\"):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ùå ERROR: Wrong NumPy version detected!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n   You have NumPy {np.__version__}, but need 1.26.x\")\n",
    "    print(\"\\n   FIX: Click Runtime ‚Üí Restart runtime, then run THIS cell again\")\n",
    "    print(\"=\"*70)\n",
    "    raise RuntimeError(\"Please restart the runtime and try again!\")\n",
    "\n",
    "print(f\"‚úì NumPy version: {np.__version__} (compatible)\")\n",
    "\n",
    "import sae_lens\n",
    "print(f\"‚úì sae-lens version: {sae_lens.__version__}\")\n",
    "\n",
    "import torch\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "import transformers\n",
    "print(f\"‚úì transformers version: {transformers.__version__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ All packages loaded successfully!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1aa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: HuggingFace Login + Google Drive Mounting (COLAB ONLY)\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive to save results\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "print(\"‚úì Google Drive mounted at /content/drive\")\n",
    "\n",
    "# Setup HuggingFace authentication\n",
    "from huggingface_hub import login\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "if HF_TOKEN is None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ö†Ô∏è  Enter your HuggingFace token when prompted below\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"To get token: https://huggingface.co/settings/tokens\")\n",
    "    print(\"To accept license: https://huggingface.co/google/gemma-2-2b-it\")\n",
    "    print(\"=\"*70)\n",
    "    login()  # This will prompt for token\n",
    "else:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úì HuggingFace authenticated!\")\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc60ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: Setup Checkpoint & Result Saving to Google Drive\n",
    "# ============================================================================\n",
    "# \n",
    "# Save results during long experiments to Google Drive\n",
    "# so they persist even if the Colab session disconnects\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "class ColabExperimentCheckpoint:\n",
    "    \"\"\"Manage checkpoints and results for long-running Colab experiments\"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_name=\"default\"):\n",
    "        self.experiment_name = experiment_name\n",
    "        # Save to Google Drive so results persist after disconnect\n",
    "        self.results_dir = Path(\"/content/drive/My Drive/ECE685_Results\")\n",
    "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.results = {}\n",
    "        self.load_latest_checkpoint()\n",
    "    \n",
    "    def load_latest_checkpoint(self):\n",
    "        \"\"\"Load the most recent checkpoint if it exists\"\"\"\n",
    "        checkpoint_files = sorted(self.results_dir.glob(f\"{self.experiment_name}_*.pkl\"))\n",
    "        if checkpoint_files:\n",
    "            latest = checkpoint_files[-1]\n",
    "            with open(latest, 'rb') as f:\n",
    "                self.results = pickle.load(f)\n",
    "            print(f\"‚úì Loaded checkpoint: {latest.name}\")\n",
    "            print(f\"  Contains {len(self.results)} result entries\")\n",
    "        else:\n",
    "            print(f\"‚Ñπ No previous checkpoint found. Starting fresh.\")\n",
    "    \n",
    "    def save_result(self, key, value):\n",
    "        \"\"\"Save a single result\"\"\"\n",
    "        self.results[key] = value\n",
    "        print(f\"  ‚Üí Saved: {key}\")\n",
    "    \n",
    "    def save_batch(self, batch_dict):\n",
    "        \"\"\"Save multiple results at once\"\"\"\n",
    "        self.results.update(batch_dict)\n",
    "        print(f\"  ‚Üí Saved {len(batch_dict)} results\")\n",
    "    \n",
    "    def checkpoint(self):\n",
    "        \"\"\"Save current results to Google Drive\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        checkpoint_path = self.results_dir / f\"{self.experiment_name}_{timestamp}.pkl\"\n",
    "        \n",
    "        with open(checkpoint_path, 'wb') as f:\n",
    "            pickle.dump(self.results, f)\n",
    "        \n",
    "        print(f\"‚úì Checkpoint saved to Google Drive: {checkpoint_path.name}\")\n",
    "        print(f\"  Path: {checkpoint_path}\")\n",
    "        return checkpoint_path\n",
    "    \n",
    "    def get_result(self, key, default=None):\n",
    "        \"\"\"Retrieve a saved result\"\"\"\n",
    "        return self.results.get(key, default)\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print summary of saved results\"\"\"\n",
    "        print(f\"\\nüìä Experiment Summary: {self.experiment_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        if self.results:\n",
    "            for i, key in enumerate(self.results.keys(), 1):\n",
    "                print(f\"  {i}. {key}\")\n",
    "        else:\n",
    "            print(\"  (No results saved yet)\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Create checkpoint manager for this experiment\n",
    "checkpoint = ColabExperimentCheckpoint(experiment_name=\"multilayer_sae\")\n",
    "\n",
    "print(\"\\n‚úÖ Checkpoint system ready!\")\n",
    "print(\"\\nUsage examples:\")\n",
    "print(\"  checkpoint.save_result('layer_12_features', features)\")\n",
    "print(\"  checkpoint.save_batch({'key1': val1, 'key2': val2})\")\n",
    "print(\"  checkpoint.checkpoint()  # Save to Google Drive\")\n",
    "print(\"  checkpoint.get_result('key')\")\n",
    "print(f\"\\nResults saved to: {checkpoint.results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab238bd",
   "metadata": {},
   "source": [
    "## üì¶ Multi-Layer Configuration\n",
    "\n",
    "**Key Change**: We now configure TWO layers (12 and 20) instead of just one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16729f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Multi-Layer Config loaded\n",
      "  Model: google/gemma-2-2b-it\n",
      "  Hook Layers: [12, 20]\n",
      "  SAE IDs:\n",
      "    Layer 12: layer_12/width_16k/canonical\n",
      "    Layer 20: layer_20/width_16k/canonical\n",
      "  Data limits: NQ=1000, HH=1000, RTP=1000\n",
      "  Combined feature dim: 32768 (16k √ó 2 layers)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MULTI-LAYER CONFIGURATION\n",
    "# ============================================================================\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Literal, Optional, Dict, Any, List\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class MultiLayerConfig:\n",
    "    \"\"\"Configuration for multi-layer SAE experiment\"\"\"\n",
    "    \n",
    "    # Model settings\n",
    "    gemma_model_name: str = \"google/gemma-2-2b-it\"\n",
    "    toxicity_model_name: str = \"unitary/unbiased-toxic-roberta\"\n",
    "    dtype: str = \"bfloat16\"\n",
    "    \n",
    "    # ========== MULTI-LAYER SETTINGS ==========\n",
    "    # Hook at BOTH layers 12 (mid) and 20 (deep)\n",
    "    hook_layers: List[int] = field(default_factory=lambda: [12, 20])\n",
    "    \n",
    "    # SAE settings for EACH layer (Gemma Scope)\n",
    "    sae_release: str = \"gemma-scope-2b-pt-res-canonical\"\n",
    "    sae_ids: Dict[int, str] = field(default_factory=lambda: {\n",
    "        12: \"layer_12/width_16k/canonical\",\n",
    "        20: \"layer_20/width_16k/canonical\",\n",
    "    })\n",
    "    \n",
    "    # Data settings - smaller for faster experiments\n",
    "    nq_sample_limit: int = 1000  # Reduced for speed (vs 3610)\n",
    "    hh_sample_limit: int = 1000  # Reduced for speed (vs 8552)\n",
    "    rtp_sample_limit: int = 1000  # Reduced for speed\n",
    "    \n",
    "    # Batch sizes\n",
    "    data_batch_size: int = 32  # Smaller batch for multi-layer (more memory)\n",
    "    steering_batch_size: int = 8\n",
    "    \n",
    "    # Steering settings\n",
    "    steering_samples: int = 100  # Per condition\n",
    "    steering_strength_ratios: list = field(default_factory=lambda: [0.0, 0.1, 0.2, 0.3])\n",
    "    \n",
    "    # Feature discovery\n",
    "    top_k_features: int = 100\n",
    "    \n",
    "    # Experiment\n",
    "    device: str = \"cuda\"\n",
    "    seed: int = 42\n",
    "\n",
    "CONFIG = MultiLayerConfig()\n",
    "\n",
    "print(\"‚úì Multi-Layer Config loaded\")\n",
    "print(f\"  Model: {CONFIG.gemma_model_name}\")\n",
    "print(f\"  Hook Layers: {CONFIG.hook_layers}\")\n",
    "print(f\"  SAE IDs:\")\n",
    "for layer, sae_id in CONFIG.sae_ids.items():\n",
    "    print(f\"    Layer {layer}: {sae_id}\")\n",
    "print(f\"  Data limits: NQ={CONFIG.nq_sample_limit}, HH={CONFIG.hh_sample_limit}, RTP={CONFIG.rtp_sample_limit}\")\n",
    "print(f\"  Combined feature dim: {16384 * len(CONFIG.hook_layers)} (16k √ó {len(CONFIG.hook_layers)} layers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37caebac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Multi-Layer SAE classes defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MULTI-LAYER SAE WRAPPER\n",
    "# ============================================================================\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "\n",
    "@dataclass\n",
    "class SparseAutoencoder:\n",
    "    \"\"\"Wrapper for a single Gemma Scope SAE\"\"\"\n",
    "    encoder_weight: torch.Tensor\n",
    "    decoder_weight: torch.Tensor\n",
    "    bias: torch.Tensor\n",
    "    layer: int\n",
    "    n_features: int\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, layer: int, sae_id: str) -> \"SparseAutoencoder\":\n",
    "        \"\"\"Load pretrained SAE for a specific layer\"\"\"\n",
    "        print(f\"  Loading SAE for Layer {layer}: {sae_id}\")\n",
    "        \n",
    "        sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "            release=CONFIG.sae_release,\n",
    "            sae_id=sae_id,\n",
    "            device=\"cpu\",\n",
    "        )\n",
    "        \n",
    "        encoder_w = sae.W_enc.detach().cpu().t()\n",
    "        decoder_w = sae.W_dec.detach().cpu()\n",
    "        \n",
    "        print(f\"    ‚úì Layer {layer} SAE: {sae.cfg.d_sae} features, d_in={sae.cfg.d_in}\")\n",
    "        \n",
    "        return cls(\n",
    "            encoder_weight=encoder_w,\n",
    "            decoder_weight=decoder_w,\n",
    "            bias=sae.b_enc.detach().cpu() if hasattr(sae, \"b_enc\") else torch.zeros(sae.cfg.d_sae),\n",
    "            layer=layer,\n",
    "            n_features=sae.cfg.d_sae,\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def encode(self, residual: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encode residual to sparse codes\"\"\"\n",
    "        residual = residual.float().cpu()\n",
    "        if residual.dim() == 3:\n",
    "            residual = residual.reshape(-1, residual.size(-1))\n",
    "        projected = torch.nn.functional.linear(residual, self.encoder_weight, self.bias)\n",
    "        return torch.nn.functional.relu(projected)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def decode(self, codes: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Decode codes back to residual space\"\"\"\n",
    "        return torch.nn.functional.linear(codes, self.decoder_weight.t())\n",
    "\n",
    "\n",
    "class MultiLayerSAE:\n",
    "    \"\"\"Manages SAEs for multiple layers and concatenates features\"\"\"\n",
    "    \n",
    "    def __init__(self, layers: List[int], sae_ids: Dict[int, str]):\n",
    "        self.layers = sorted(layers)\n",
    "        self.saes = {}\n",
    "        \n",
    "        print(\"Loading Multi-Layer SAEs...\")\n",
    "        for layer in self.layers:\n",
    "            self.saes[layer] = SparseAutoencoder.load(layer, sae_ids[layer])\n",
    "        \n",
    "        self.total_features = sum(sae.n_features for sae in self.saes.values())\n",
    "        print(f\"‚úì Multi-Layer SAE ready: {len(self.layers)} layers, {self.total_features} total features\")\n",
    "    \n",
    "    def encode_single(self, residuals: Dict[int, torch.Tensor], layer: int) -> torch.Tensor:\n",
    "        \"\"\"Encode residual from a single layer\"\"\"\n",
    "        return self.saes[layer].encode(residuals[layer])\n",
    "    \n",
    "    def encode_concat(self, residuals: Dict[int, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"Encode and concatenate features from all layers\"\"\"\n",
    "        codes_list = []\n",
    "        for layer in self.layers:\n",
    "            codes = self.saes[layer].encode(residuals[layer])\n",
    "            codes_list.append(codes)\n",
    "        return torch.cat(codes_list, dim=-1)\n",
    "    \n",
    "    def build_steering_vector(self, layer: int, f_plus_ids: list, f_minus_ids: list,\n",
    "                               plus_weight: float = -1.0, minus_weight: float = 0.5) -> torch.Tensor:\n",
    "        \"\"\"Build steering vector for a specific layer\"\"\"\n",
    "        sae = self.saes[layer]\n",
    "        d_in = sae.decoder_weight.shape[1]\n",
    "        direction = torch.zeros(d_in)\n",
    "        \n",
    "        for idx in f_plus_ids:\n",
    "            if idx < sae.n_features:\n",
    "                direction += plus_weight * sae.decoder_weight[idx]\n",
    "        for idx in f_minus_ids:\n",
    "            if idx < sae.n_features:\n",
    "                direction += minus_weight * sae.decoder_weight[idx]\n",
    "        \n",
    "        if direction.norm() > 0:\n",
    "            direction = direction / direction.norm()\n",
    "        return direction\n",
    "\n",
    "print(\"‚úì Multi-Layer SAE classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f85ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì MultiLayerGemma class defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MULTI-LAYER GEMMA INTERFACE\n",
    "# ============================================================================\n",
    "from contextlib import contextmanager\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class MultiLayerCapture:\n",
    "    \"\"\"Captures residual stream from MULTIPLE layers\"\"\"\n",
    "    \n",
    "    def __init__(self, layers: List[int]):\n",
    "        self.layers = layers\n",
    "        self.residuals = {layer: None for layer in layers}\n",
    "    \n",
    "    def make_hook(self, layer: int):\n",
    "        def hook(module, inputs, output):\n",
    "            hidden = output[0] if isinstance(output, tuple) else output\n",
    "            self.residuals[layer] = hidden.detach()\n",
    "        return hook\n",
    "    \n",
    "    def clear(self):\n",
    "        self.residuals = {layer: None for layer in self.layers}\n",
    "\n",
    "\n",
    "class MultiLayerGemma:\n",
    "    \"\"\"Gemma with multi-layer activation capture\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = None):\n",
    "        model_id = model_name or CONFIG.gemma_model_name\n",
    "        dtype = getattr(torch, CONFIG.dtype)\n",
    "        \n",
    "        print(f\"Loading {model_id}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        self.capture = MultiLayerCapture(CONFIG.hook_layers)\n",
    "        self._handles = []\n",
    "        print(f\"‚úì Gemma loaded on {self.model.device}\")\n",
    "        print(f\"  Capturing layers: {CONFIG.hook_layers}\")\n",
    "    \n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register hooks for all target layers\"\"\"\n",
    "        self.remove_hooks()\n",
    "        for layer in self.capture.layers:\n",
    "            block = self.model.model.layers[layer]\n",
    "            handle = block.register_forward_hook(self.capture.make_hook(layer))\n",
    "            self._handles.append(handle)\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for handle in self._handles:\n",
    "            handle.remove()\n",
    "        self._handles = []\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate_with_capture(self, prompt: str, max_new_tokens: int = 50) -> dict:\n",
    "        \"\"\"Generate text and capture residuals from all layers\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        self.capture.clear()\n",
    "        self.register_hooks()\n",
    "        \n",
    "        try:\n",
    "            output_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "        finally:\n",
    "            self.remove_hooks()\n",
    "        \n",
    "        text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Get last-token residuals from each layer\n",
    "        residuals = {}\n",
    "        for layer, res in self.capture.residuals.items():\n",
    "            if res is not None:\n",
    "                residuals[layer] = res[:, -1, :].cpu()\n",
    "        \n",
    "        return {\"text\": text, \"residuals\": residuals}\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate_batch_with_capture(self, prompts: list, max_new_tokens: int = 50) -> list:\n",
    "        \"\"\"Batch generate with multi-layer residual capture\"\"\"\n",
    "        if not prompts:\n",
    "            return []\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        # Store residuals per layer\n",
    "        batch_residuals = {layer: [] for layer in self.capture.layers}\n",
    "        \n",
    "        def make_batch_hook(layer):\n",
    "            def hook(module, inp, output):\n",
    "                hidden = output[0] if isinstance(output, tuple) else output\n",
    "                batch_residuals[layer].append(hidden.detach().cpu())\n",
    "            return hook\n",
    "        \n",
    "        # Register hooks\n",
    "        handles = []\n",
    "        for layer in self.capture.layers:\n",
    "            block = self.model.model.layers[layer]\n",
    "            handle = block.register_forward_hook(make_batch_hook(layer))\n",
    "            handles.append(handle)\n",
    "        \n",
    "        try:\n",
    "            output_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "        finally:\n",
    "            for handle in handles:\n",
    "                handle.remove()\n",
    "        \n",
    "        texts = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Build results\n",
    "        results = []\n",
    "        for i, text in enumerate(texts):\n",
    "            residuals = {}\n",
    "            for layer in self.capture.layers:\n",
    "                if batch_residuals[layer]:\n",
    "                    hidden = batch_residuals[layer][0]  # First forward pass\n",
    "                    if i < hidden.shape[0]:\n",
    "                        seq_len = (inputs['attention_mask'][i] == 1).sum().item()\n",
    "                        residuals[layer] = hidden[i, seq_len-1, :].unsqueeze(0)\n",
    "            \n",
    "            results.append({\"text\": text, \"residuals\": residuals})\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"‚úì MultiLayerGemma class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5336e3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ToxicityWrapper class defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TOXICITY CLASSIFIER\n",
    "# ============================================================================\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "@dataclass\n",
    "class ToxicityScore:\n",
    "    probability: float\n",
    "    label: int\n",
    "\n",
    "class ToxicityWrapper:\n",
    "    def __init__(self, model_name: str = None):\n",
    "        model_id = model_name or CONFIG.toxicity_model_name\n",
    "        print(f\"Loading toxicity classifier: {model_id}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "        self.model.eval()\n",
    "        print(\"‚úì Toxicity classifier loaded\")\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def score(self, text: str, threshold: float = 0.5) -> ToxicityScore:\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return ToxicityScore(probability=0.0, label=0)\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        logits = self.model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        prob = probs[0, 1].item()\n",
    "        return ToxicityScore(probability=prob, label=int(prob >= threshold))\n",
    "\n",
    "print(\"‚úì ToxicityWrapper class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26f7350",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üöÄ Part 1: Load Models\n",
    "\n",
    "Now we load:\n",
    "1. **Gemma-2B-IT** with multi-layer hooks\n",
    "2. **SAEs for BOTH layers 12 and 20**\n",
    "3. **Toxicity Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2a098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING MODELS (MULTI-LAYER)\n",
      "============================================================\n",
      "Loading google/gemma-2-2b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9777380535a04a39a6cc3218a5c2df79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f661f3676f864f0fa8b9ec17e909bc58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1c43a818444babb07d3e455691675a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25647a54b2ad483d8c5b5ec52e567d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8336fff2254e2ea5d80157a3e004b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2dc506cbef342c7b0a15a262f7242cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ca9d8ae3a740318504c82172a70b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210689324f7b4db1a806ccb6fc940c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2226a1d607e8436d98ec3cceb1876ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c550b7a656442d9d22ec7c9f9bece6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565d6d3a47ea45b38727c2ee6ecc0347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Gemma loaded on cpu\n",
      "  Capturing layers: [12, 20]\n",
      "  Hidden size: 2304\n",
      "\n",
      "Loading Multi-Layer SAEs...\n",
      "  Loading SAE for Layer 12: layer_12/width_16k/canonical\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d29981b7154f1e9305810a28578fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_12/width_16k/average_l0_82/params.(‚Ä¶):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-947390636.py:21: DeprecationWarning: Unpacking SAE objects is deprecated. SAE.from_pretrained() now returns only the SAE object. Use SAE.from_pretrained_with_cfg_and_sparsity() to get the config dict and sparsity as well.\n",
      "  sae, cfg_dict, sparsity = SAE.from_pretrained(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì Layer 12 SAE: 16384 features, d_in=2304\n",
      "  Loading SAE for Layer 20: layer_20/width_16k/canonical\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bb69cb512d43b79939f930884df7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layer_20/width_16k/average_l0_71/params.(‚Ä¶):   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úì Layer 20 SAE: 16384 features, d_in=2304\n",
      "‚úì Multi-Layer SAE ready: 2 layers, 32768 total features\n",
      "\n",
      "Loading toxicity classifier: unitary/unbiased-toxic-roberta...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c618c62a576e4676b20e3db248ed84a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/997 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bca3b1aabb0472691660ad6b02ea86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc594b14dcc047149d5bfc0c65118b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b8ee763dd643d19cdaaf4704fa1ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d16191b535414cad2eb3cece26a709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4ba4026b814f05a862c0ee4ce4f02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Toxicity classifier loaded\n",
      "\n",
      "============================================================\n",
      "‚úì ALL MODELS LOADED!\n",
      "  Gemma: google/gemma-2-2b-it\n",
      "  SAE Layers: [12, 20]\n",
      "  Total SAE features: 32768\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1778c721f54946558553d5a981262247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD ALL MODELS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING MODELS (MULTI-LAYER)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Load Gemma with multi-layer capture\n",
    "gemma = MultiLayerGemma()\n",
    "hidden_size = gemma.model.config.hidden_size\n",
    "print(f\"  Hidden size: {hidden_size}\")\n",
    "\n",
    "# 2. Load SAEs for BOTH layers\n",
    "print()\n",
    "multi_sae = MultiLayerSAE(CONFIG.hook_layers, CONFIG.sae_ids)\n",
    "\n",
    "# 3. Load toxicity classifier\n",
    "print()\n",
    "tox = ToxicityWrapper()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì ALL MODELS LOADED!\")\n",
    "print(f\"  Gemma: {CONFIG.gemma_model_name}\")\n",
    "print(f\"  SAE Layers: {CONFIG.hook_layers}\")\n",
    "print(f\"  Total SAE features: {multi_sae.total_features}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0fa617",
   "metadata": {},
   "source": [
    "## üìä Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f4499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets from HuggingFace...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914687ecea3e43d2ade91dbe22fc403a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15388f90ffbf48188b82c7a7f221d061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "nq_open/train-00000-of-00001.parquet:   0%|          | 0.00/4.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226a9c2ea041412993c43d2b05055329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "nq_open/validation-00000-of-00001.parque(‚Ä¶):   0%|          | 0.00/214k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd7a4b2bfe842ca9e6e4b7e7d3d6615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87925 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62f83739858400bac5ed6636521c2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì NQ-Open: 3610 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad25690e0edc4355b6fb124527475dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad38d8389e1d42939875acc4edfd1286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prompts.jsonl:   0%|          | 0.00/67.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1befa6ba4c89400fbffc2cf537bff0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/99442 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì RealToxicityPrompts: 99442 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941efe7a49f14b8cb97beffe58bb96d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7663af957c14ae0a17cc3d12a7fa3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "harmless-base/train.jsonl.gz:   0%|          | 0.00/13.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dbee79bbaee4ecaa502e4283d8af4b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "helpful-base/train.jsonl.gz:   0%|          | 0.00/16.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8b3e5014cd41e986a1723f1aec223d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "helpful-online/train.jsonl.gz:   0%|          | 0.00/20.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1816592b17bb46288fe08c7b779ecbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "helpful-rejection-sampled/train.jsonl.gz:   0%|          | 0.00/25.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fe7c5d01b04f108e541515e1f25a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "harmless-base/test.jsonl.gz:   0%|          | 0.00/743k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed628ee546a4507a6043f447873dda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "helpful-base/test.jsonl.gz:   0%|          | 0.00/875k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6050f9b9dc8143ea9474cec875dcd271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "helpful-online/test.jsonl.gz:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55dde8a61824ed68a6e3e5addb8abfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "helpful-rejection-sampled/test.jsonl.gz:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1413117424db41f1b5cfa570213cdc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/160800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9a717bb0534c8faa376b1fd00ef16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/8552 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Anthropic HH: 8552 samples\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Loading datasets from HuggingFace...\")\n",
    "\n",
    "# NQ-Open: Question answering with ground truth\n",
    "nq_dataset = load_dataset(\"nq_open\", split=\"validation\")\n",
    "print(f\"‚úì NQ-Open: {len(nq_dataset)} samples\")\n",
    "\n",
    "# RealToxicityPrompts\n",
    "rtp_dataset = load_dataset(\"allenai/real-toxicity-prompts\", split=\"train\")\n",
    "print(f\"‚úì RealToxicityPrompts: {len(rtp_dataset)} samples\")\n",
    "\n",
    "# Anthropic HH: Safe conversations\n",
    "hh_dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"test\")\n",
    "print(f\"‚úì Anthropic HH: {len(hh_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87468761",
   "metadata": {},
   "source": [
    "## üî¨ Multi-Layer Activation Capture & Labeling\n",
    "\n",
    "For each prompt, we now:\n",
    "1. Run through Gemma and capture residuals from **BOTH layers 12 and 20**\n",
    "2. Encode with respective SAEs\n",
    "3. Store **BOTH** single-layer codes AND concatenated codes for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b633f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def check_hallucination(model_answer: str, ground_truth_answers: list) -> bool:\n",
    "    \"\"\"Check if model answer is a hallucination\"\"\"\n",
    "    model_answer_lower = model_answer.lower().strip()\n",
    "    \n",
    "    if len(model_answer_lower.split()) < 2:\n",
    "        return True\n",
    "    \n",
    "    for gt in ground_truth_answers:\n",
    "        gt_lower = gt.lower().strip()\n",
    "        if gt_lower in model_answer_lower or model_answer_lower in gt_lower:\n",
    "            return False\n",
    "        gt_words = set(gt_lower.split())\n",
    "        answer_words = set(model_answer_lower.split())\n",
    "        if len(gt_words & answer_words) >= min(2, len(gt_words)):\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def process_nq_multilayer(dataset, limit: int, batch_size: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Process NQ-Open with multi-layer SAE encoding\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = CONFIG.data_batch_size\n",
    "    \n",
    "    questions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    for i, row in enumerate(dataset):\n",
    "        if i >= limit:\n",
    "            break\n",
    "        questions.append(row['question'])\n",
    "        ground_truths.append(row['answer'])\n",
    "    \n",
    "    print(f\"  Processing {len(questions)} questions...\")\n",
    "    records = []\n",
    "    \n",
    "    for batch_start in tqdm(range(0, len(questions), batch_size), desc=\"NQ-Open\"):\n",
    "        batch_end = min(batch_start + batch_size, len(questions))\n",
    "        batch_q = questions[batch_start:batch_end]\n",
    "        batch_gt = ground_truths[batch_start:batch_end]\n",
    "        \n",
    "        results = gemma.generate_batch_with_capture(batch_q, max_new_tokens=50)\n",
    "        \n",
    "        for j, (q, gt, res) in enumerate(zip(batch_q, batch_gt, results)):\n",
    "            if not res['residuals']:\n",
    "                continue\n",
    "            \n",
    "            answer = res['text'].replace(q, '').strip()\n",
    "            \n",
    "            # Encode with EACH layer's SAE\n",
    "            codes_per_layer = {}\n",
    "            for layer in CONFIG.hook_layers:\n",
    "                if layer in res['residuals']:\n",
    "                    codes_per_layer[layer] = multi_sae.encode_single(res['residuals'], layer).squeeze(0).numpy()\n",
    "            \n",
    "            # Also get concatenated codes\n",
    "            codes_concat = multi_sae.encode_concat(res['residuals']).squeeze(0).numpy()\n",
    "            \n",
    "            is_halluc = check_hallucination(answer, gt)\n",
    "            \n",
    "            records.append({\n",
    "                'id': batch_start + j,\n",
    "                'prompt': q,\n",
    "                'answer': answer,\n",
    "                'ground_truth': gt,\n",
    "                'label': int(is_halluc),\n",
    "                'codes_l12': codes_per_layer.get(12),\n",
    "                'codes_l20': codes_per_layer.get(20),\n",
    "                'codes_concat': codes_concat,\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    n_halluc = df['label'].sum()\n",
    "    print(f\"‚úì NQ: {len(df)} samples, {n_halluc} hallucinated ({n_halluc/len(df)*100:.1f}%)\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_toxicity_multilayer(dataset, name: str, limit: int, \n",
    "                                 text_field: str = 'prompt',\n",
    "                                 batch_size: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Process toxicity dataset with multi-layer SAE encoding\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = CONFIG.data_batch_size\n",
    "    \n",
    "    prompts = []\n",
    "    indices = []\n",
    "    \n",
    "    for i, row in enumerate(dataset):\n",
    "        if len(prompts) >= limit:\n",
    "            break\n",
    "        \n",
    "        if text_field == 'prompt' and isinstance(row.get('prompt'), dict):\n",
    "            prompt = row['prompt'].get('text', '')\n",
    "        elif text_field == 'chosen':\n",
    "            text = row['chosen']\n",
    "            if 'Human:' in text:\n",
    "                prompt = text.split('Human:')[1].split('Assistant:')[0].strip()\n",
    "            else:\n",
    "                prompt = text[:200]\n",
    "        else:\n",
    "            prompt = row.get(text_field, '')\n",
    "        \n",
    "        if prompt and len(prompt) >= 5:\n",
    "            prompts.append(prompt[:500])\n",
    "            indices.append(i)\n",
    "    \n",
    "    print(f\"  Processing {len(prompts)} prompts...\")\n",
    "    records = []\n",
    "    \n",
    "    for batch_start in tqdm(range(0, len(prompts), batch_size), desc=name):\n",
    "        batch_end = min(batch_start + batch_size, len(prompts))\n",
    "        batch_p = prompts[batch_start:batch_end]\n",
    "        batch_idx = indices[batch_start:batch_end]\n",
    "        \n",
    "        results = gemma.generate_batch_with_capture(batch_p, max_new_tokens=50)\n",
    "        \n",
    "        for j, (p, idx, res) in enumerate(zip(batch_p, batch_idx, results)):\n",
    "            if not res['residuals']:\n",
    "                continue\n",
    "            \n",
    "            completion = res['text'].replace(p, '').strip()\n",
    "            \n",
    "            codes_per_layer = {}\n",
    "            for layer in CONFIG.hook_layers:\n",
    "                if layer in res['residuals']:\n",
    "                    codes_per_layer[layer] = multi_sae.encode_single(res['residuals'], layer).squeeze(0).numpy()\n",
    "            \n",
    "            codes_concat = multi_sae.encode_concat(res['residuals']).squeeze(0).numpy()\n",
    "            \n",
    "            tox_score = tox.score(completion)\n",
    "            \n",
    "            records.append({\n",
    "                'id': idx,\n",
    "                'prompt': p,\n",
    "                'completion': completion,\n",
    "                'toxicity': tox_score.probability,\n",
    "                'label': tox_score.label,\n",
    "                'codes_l12': codes_per_layer.get(12),\n",
    "                'codes_l20': codes_per_layer.get(20),\n",
    "                'codes_concat': codes_concat,\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    n_toxic = df['label'].sum()\n",
    "    print(f\"‚úì {name}: {len(df)} samples, {n_toxic} toxic ({n_toxic/len(df)*100:.1f}%)\")\n",
    "    return df\n",
    "\n",
    "print(\"‚úì Multi-layer processing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae74331",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PROCESS ALL DATASETS WITH MULTI-LAYER ENCODING\n",
    "# ============================================================================\n",
    "import random\n",
    "random.seed(CONFIG.seed)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PROCESSING DATASETS (MULTI-LAYER)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Layers: {CONFIG.hook_layers}\")\n",
    "print(f\"Batch size: {CONFIG.data_batch_size}\")\n",
    "\n",
    "# 1. NQ-Open (Hallucination)\n",
    "print(\"\\n1/3: Processing NQ-Open...\")\n",
    "nq_df = process_nq_multilayer(nq_dataset, limit=CONFIG.nq_sample_limit)\n",
    "\n",
    "# 2. Anthropic HH (Safe)\n",
    "print(\"\\n2/3: Processing Anthropic HH...\")\n",
    "hh_df = process_toxicity_multilayer(hh_dataset, \"HH\", limit=CONFIG.hh_sample_limit, text_field='chosen')\n",
    "\n",
    "# 3. RealToxicityPrompts\n",
    "rtp_indices = list(range(len(rtp_dataset)))\n",
    "random.shuffle(rtp_indices)\n",
    "rtp_shuffled = rtp_dataset.select(rtp_indices[:CONFIG.rtp_sample_limit])\n",
    "\n",
    "print(\"\\n3/3: Processing RTP...\")\n",
    "rtp_df = process_toxicity_multilayer(rtp_shuffled, \"RTP\", limit=CONFIG.rtp_sample_limit, text_field='prompt')\n",
    "\n",
    "# Combine for safety task\n",
    "safety_df = pd.concat([rtp_df, hh_df], ignore_index=True)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATA READY!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Hallucination: {len(nq_df)} samples\")\n",
    "print(f\"Safety: {len(safety_df)} samples (RTP: {len(rtp_df)}, HH: {len(hh_df)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd502e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîç Part 2: Multi-Layer Feature Discovery & Detection Comparison\n",
    "\n",
    "This is the **key experiment**: We compare detection performance using:\n",
    "1. **Layer 12 only** (baseline)\n",
    "2. **Layer 20 only** (deep layer)\n",
    "3. **Layers 12+20 concatenated** (multi-layer)\n",
    "\n",
    "The hypothesis is that combining features from multiple layers should improve detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03250fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE DISCOVERY & DETECTION FUNCTIONS\n",
    "# ============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "\n",
    "def discover_features(codes_matrix: np.ndarray, labels: np.ndarray, \n",
    "                       task_name: str, top_k: int = 50):\n",
    "    \"\"\"Find correlated features\"\"\"\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    if len(unique) < 2:\n",
    "        print(f\"  ‚ö†Ô∏è {task_name}: Only one class, using variance-based selection\")\n",
    "        var = codes_matrix.var(axis=0)\n",
    "        top_idx = np.argsort(var)[-top_k:][::-1]\n",
    "        return top_idx, top_idx[:top_k//2], top_idx[top_k//2:]\n",
    "    \n",
    "    correlations = []\n",
    "    for i in range(codes_matrix.shape[1]):\n",
    "        feat = codes_matrix[:, i]\n",
    "        if feat.std() > 0:\n",
    "            corr = np.corrcoef(feat, labels)[0, 1]\n",
    "            if not np.isnan(corr):\n",
    "                correlations.append((i, corr))\n",
    "    \n",
    "    correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    f_plus = [c[0] for c in correlations[:top_k] if c[1] > 0][:top_k//2]\n",
    "    f_minus = [c[0] for c in correlations[:top_k] if c[1] < 0][:top_k//2]\n",
    "    top_all = [c[0] for c in correlations[:top_k]]\n",
    "    \n",
    "    return top_all, f_plus, f_minus\n",
    "\n",
    "\n",
    "def train_and_evaluate(codes_matrix: np.ndarray, labels: np.ndarray, \n",
    "                        feature_indices: list, name: str):\n",
    "    \"\"\"Train detector and return metrics\"\"\"\n",
    "    X = codes_matrix[:, feature_indices] if feature_indices else codes_matrix\n",
    "    y = labels\n",
    "    \n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    if len(unique) < 2:\n",
    "        return {'accuracy': 1.0, 'f1': 0.0, 'auroc': 0.5}\n",
    "    \n",
    "    min_class = min(counts)\n",
    "    stratify = y if min_class >= 5 else None\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=CONFIG.seed, stratify=stratify\n",
    "    )\n",
    "    \n",
    "    clf = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=CONFIG.seed)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'auroc': roc_auc_score(y_test, y_prob) if len(np.unique(y_test)) > 1 else 0.5\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"‚úì Detection functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e17f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPARE SINGLE-LAYER vs MULTI-LAYER DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LAYER COMPARISON EXPERIMENT\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nComparing detection performance across layer configurations...\")\n",
    "\n",
    "# Store results\n",
    "results = {\n",
    "    'hallucination': {},\n",
    "    'safety': {}\n",
    "}\n",
    "\n",
    "TOP_K = 50  # Features to use\n",
    "\n",
    "# ==================== HALLUCINATION TASK ====================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"HALLUCINATION DETECTION\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "halluc_labels = nq_df['label'].values\n",
    "\n",
    "# Layer 12 only\n",
    "codes_l12 = np.stack([c for c in nq_df['codes_l12'].values if c is not None])\n",
    "top_l12, fp_l12, fm_l12 = discover_features(codes_l12, halluc_labels[:len(codes_l12)], \"Halluc-L12\", TOP_K)\n",
    "metrics_l12 = train_and_evaluate(codes_l12, halluc_labels[:len(codes_l12)], top_l12, \"L12\")\n",
    "results['hallucination']['layer_12'] = metrics_l12\n",
    "print(f\"\\n  Layer 12:  Acc={metrics_l12['accuracy']:.3f}, F1={metrics_l12['f1']:.3f}, AUC={metrics_l12['auroc']:.3f}\")\n",
    "\n",
    "# Layer 20 only\n",
    "codes_l20 = np.stack([c for c in nq_df['codes_l20'].values if c is not None])\n",
    "top_l20, fp_l20, fm_l20 = discover_features(codes_l20, halluc_labels[:len(codes_l20)], \"Halluc-L20\", TOP_K)\n",
    "metrics_l20 = train_and_evaluate(codes_l20, halluc_labels[:len(codes_l20)], top_l20, \"L20\")\n",
    "results['hallucination']['layer_20'] = metrics_l20\n",
    "print(f\"  Layer 20:  Acc={metrics_l20['accuracy']:.3f}, F1={metrics_l20['f1']:.3f}, AUC={metrics_l20['auroc']:.3f}\")\n",
    "\n",
    "# Multi-layer (concatenated)\n",
    "codes_concat = np.stack([c for c in nq_df['codes_concat'].values if c is not None])\n",
    "top_concat, fp_concat, fm_concat = discover_features(codes_concat, halluc_labels[:len(codes_concat)], \"Halluc-Multi\", TOP_K)\n",
    "metrics_multi = train_and_evaluate(codes_concat, halluc_labels[:len(codes_concat)], top_concat, \"Multi\")\n",
    "results['hallucination']['multi_layer'] = metrics_multi\n",
    "print(f\"  Multi-Layer: Acc={metrics_multi['accuracy']:.3f}, F1={metrics_multi['f1']:.3f}, AUC={metrics_multi['auroc']:.3f}\")\n",
    "\n",
    "# ==================== SAFETY TASK ====================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"SAFETY DETECTION\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "safety_labels = safety_df['label'].values\n",
    "\n",
    "# Layer 12 only\n",
    "codes_l12_s = np.stack([c for c in safety_df['codes_l12'].values if c is not None])\n",
    "top_l12_s, _, _ = discover_features(codes_l12_s, safety_labels[:len(codes_l12_s)], \"Safety-L12\", TOP_K)\n",
    "metrics_l12_s = train_and_evaluate(codes_l12_s, safety_labels[:len(codes_l12_s)], top_l12_s, \"L12\")\n",
    "results['safety']['layer_12'] = metrics_l12_s\n",
    "print(f\"\\n  Layer 12:  Acc={metrics_l12_s['accuracy']:.3f}, F1={metrics_l12_s['f1']:.3f}, AUC={metrics_l12_s['auroc']:.3f}\")\n",
    "\n",
    "# Layer 20 only\n",
    "codes_l20_s = np.stack([c for c in safety_df['codes_l20'].values if c is not None])\n",
    "top_l20_s, _, _ = discover_features(codes_l20_s, safety_labels[:len(codes_l20_s)], \"Safety-L20\", TOP_K)\n",
    "metrics_l20_s = train_and_evaluate(codes_l20_s, safety_labels[:len(codes_l20_s)], top_l20_s, \"L20\")\n",
    "results['safety']['layer_20'] = metrics_l20_s\n",
    "print(f\"  Layer 20:  Acc={metrics_l20_s['accuracy']:.3f}, F1={metrics_l20_s['f1']:.3f}, AUC={metrics_l20_s['auroc']:.3f}\")\n",
    "\n",
    "# Multi-layer\n",
    "codes_concat_s = np.stack([c for c in safety_df['codes_concat'].values if c is not None])\n",
    "top_concat_s, _, _ = discover_features(codes_concat_s, safety_labels[:len(codes_concat_s)], \"Safety-Multi\", TOP_K)\n",
    "metrics_multi_s = train_and_evaluate(codes_concat_s, safety_labels[:len(codes_concat_s)], top_concat_s, \"Multi\")\n",
    "results['safety']['multi_layer'] = metrics_multi_s\n",
    "print(f\"  Multi-Layer: Acc={metrics_multi_s['accuracy']:.3f}, F1={metrics_multi_s['f1']:.3f}, AUC={metrics_multi_s['auroc']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f01fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION: LAYER COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Prepare data for plotting\n",
    "layers = ['Layer 12', 'Layer 20', 'Multi-Layer']\n",
    "metrics_names = ['Accuracy', 'F1 Score', 'ROC-AUC']\n",
    "\n",
    "halluc_data = [\n",
    "    [results['hallucination']['layer_12']['accuracy'],\n",
    "     results['hallucination']['layer_20']['accuracy'],\n",
    "     results['hallucination']['multi_layer']['accuracy']],\n",
    "    [results['hallucination']['layer_12']['f1'],\n",
    "     results['hallucination']['layer_20']['f1'],\n",
    "     results['hallucination']['multi_layer']['f1']],\n",
    "    [results['hallucination']['layer_12']['auroc'],\n",
    "     results['hallucination']['layer_20']['auroc'],\n",
    "     results['hallucination']['multi_layer']['auroc']],\n",
    "]\n",
    "\n",
    "safety_data = [\n",
    "    [results['safety']['layer_12']['accuracy'],\n",
    "     results['safety']['layer_20']['accuracy'],\n",
    "     results['safety']['multi_layer']['accuracy']],\n",
    "    [results['safety']['layer_12']['f1'],\n",
    "     results['safety']['layer_20']['f1'],\n",
    "     results['safety']['multi_layer']['f1']],\n",
    "    [results['safety']['layer_12']['auroc'],\n",
    "     results['safety']['layer_20']['auroc'],\n",
    "     results['safety']['multi_layer']['auroc']],\n",
    "]\n",
    "\n",
    "x = np.arange(len(layers))\n",
    "width = 0.35\n",
    "\n",
    "for idx, (ax, metric_name) in enumerate(zip(axes, metrics_names)):\n",
    "    bars1 = ax.bar(x - width/2, halluc_data[idx], width, label='Hallucination', color='#3498db', edgecolor='black')\n",
    "    bars2 = ax.bar(x + width/2, safety_data[idx], width, label='Safety', color='#e74c3c', edgecolor='black')\n",
    "    \n",
    "    ax.set_ylabel(metric_name)\n",
    "    ax.set_title(f'{metric_name} by Layer Configuration')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(layers)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.axhline(0.5, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars1:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'{bar.get_height():.2f}', ha='center', fontsize=9)\n",
    "    for bar in bars2:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{bar.get_height():.2f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.suptitle('Single-Layer vs Multi-Layer SAE Detection Performance', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('multilayer_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Saved multilayer_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274662e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SUMMARY TABLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTI-LAYER EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Configuration':<15} {'Task':<15} {'Accuracy':<12} {'F1 Score':<12} {'ROC-AUC':<12}\")\n",
    "print(\"-\"*66)\n",
    "\n",
    "for task in ['hallucination', 'safety']:\n",
    "    for config in ['layer_12', 'layer_20', 'multi_layer']:\n",
    "        m = results[task][config]\n",
    "        config_display = config.replace('_', ' ').title()\n",
    "        print(f\"{config_display:<15} {task.title():<15} {m['accuracy']:<12.3f} {m['f1']:<12.3f} {m['auroc']:<12.3f}\")\n",
    "    print()\n",
    "\n",
    "# Calculate improvements\n",
    "print(\"-\"*66)\n",
    "print(\"MULTI-LAYER IMPROVEMENT vs LAYER 12 BASELINE:\")\n",
    "print(\"-\"*66)\n",
    "\n",
    "for task in ['hallucination', 'safety']:\n",
    "    base_auc = results[task]['layer_12']['auroc']\n",
    "    multi_auc = results[task]['multi_layer']['auroc']\n",
    "    improvement = (multi_auc - base_auc) / base_auc * 100 if base_auc > 0 else 0\n",
    "    \n",
    "    l20_auc = results[task]['layer_20']['auroc']\n",
    "    l20_improvement = (l20_auc - base_auc) / base_auc * 100 if base_auc > 0 else 0\n",
    "    \n",
    "    print(f\"\\n  {task.title()}:\")\n",
    "    print(f\"    Layer 12 (baseline): AUC = {base_auc:.3f}\")\n",
    "    print(f\"    Layer 20:            AUC = {l20_auc:.3f} ({l20_improvement:+.1f}%)\")\n",
    "    print(f\"    Multi-Layer:         AUC = {multi_auc:.3f} ({improvement:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee426e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéÆ Part 3: Multi-Layer Steering (Optional)\n",
    "\n",
    "Test if steering at multiple layers simultaneously improves results.\n",
    "\n",
    "**Strategy**: Apply steering vectors at BOTH layer 12 and layer 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ed6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MULTI-LAYER STEERING EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "# Build steering vectors for each layer\n",
    "print(\"Building steering vectors for multi-layer steering...\")\n",
    "\n",
    "# Use discovered F+ and F- features\n",
    "N_STEER = 25\n",
    "\n",
    "# For hallucination - we need to map concat indices back to layer-specific\n",
    "# For simplicity, just use the layer-specific features we discovered earlier\n",
    "steering_vectors = {\n",
    "    12: multi_sae.build_steering_vector(12, list(fp_l12[:N_STEER]), list(fm_l12[:N_STEER])),\n",
    "    20: multi_sae.build_steering_vector(20, list(fp_l20[:N_STEER]), list(fm_l20[:N_STEER])),\n",
    "}\n",
    "\n",
    "print(f\"‚úì Steering vectors built for layers {list(steering_vectors.keys())}\")\n",
    "\n",
    "\n",
    "def generate_with_multilayer_steering(gemma_model, prompt: str, \n",
    "                                       steering_vectors: dict,\n",
    "                                       alphas: dict,\n",
    "                                       max_new_tokens: int = 50):\n",
    "    \"\"\"Generate with steering at multiple layers\"\"\"\n",
    "    inputs = gemma_model.tokenizer(prompt, return_tensors=\"pt\").to(gemma_model.model.device)\n",
    "    \n",
    "    handles = []\n",
    "    \n",
    "    # Register steering hooks for each layer\n",
    "    for layer, steer_vec in steering_vectors.items():\n",
    "        alpha = alphas.get(layer, 0.0)\n",
    "        if alpha > 0:\n",
    "            def make_steering_hook(sv, a):\n",
    "                def hook(module, inp, output):\n",
    "                    hidden = output[0] if isinstance(output, tuple) else output\n",
    "                    shifted = hidden.clone()\n",
    "                    steer = sv.to(shifted.device).to(shifted.dtype) * a\n",
    "                    shifted[:, -1, :] += steer\n",
    "                    if isinstance(output, tuple):\n",
    "                        return (shifted,) + output[1:]\n",
    "                    return shifted\n",
    "                return hook\n",
    "            \n",
    "            block = gemma_model.model.model.layers[layer]\n",
    "            handle = block.register_forward_hook(make_steering_hook(steer_vec, alpha))\n",
    "            handles.append(handle)\n",
    "    \n",
    "    try:\n",
    "        output_ids = gemma_model.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=gemma_model.tokenizer.eos_token_id,\n",
    "        )\n",
    "    finally:\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "    \n",
    "    return gemma_model.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"‚úì Multi-layer steering function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec62b659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN MULTI-LAYER STEERING COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MULTI-LAYER STEERING EXPERIMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test on a subset of prompts\n",
    "test_prompts = nq_df['prompt'].tolist()[:CONFIG.steering_samples]\n",
    "test_gt = nq_df['ground_truth'].tolist()[:CONFIG.steering_samples]\n",
    "\n",
    "# Steering conditions to compare\n",
    "# Use moderate alpha values (10-20% of typical residual norm ~150)\n",
    "ALPHA_VALUE = 15.0  # Moderate strength\n",
    "\n",
    "steering_conditions = {\n",
    "    'baseline': {12: 0.0, 20: 0.0},\n",
    "    'layer_12_only': {12: ALPHA_VALUE, 20: 0.0},\n",
    "    'layer_20_only': {12: 0.0, 20: ALPHA_VALUE},\n",
    "    'both_layers': {12: ALPHA_VALUE, 20: ALPHA_VALUE},\n",
    "}\n",
    "\n",
    "steering_results = {}\n",
    "\n",
    "for condition_name, alphas in steering_conditions.items():\n",
    "    print(f\"\\n--- Testing: {condition_name} ---\")\n",
    "    print(f\"    Alphas: L12={alphas[12]:.1f}, L20={alphas[20]:.1f}\")\n",
    "    \n",
    "    halluc_count = 0\n",
    "    \n",
    "    for prompt, gt in tqdm(zip(test_prompts, test_gt), total=len(test_prompts), desc=condition_name):\n",
    "        text = generate_with_multilayer_steering(gemma, prompt, steering_vectors, alphas)\n",
    "        answer = text.replace(prompt, '').strip()\n",
    "        \n",
    "        is_halluc = check_hallucination(answer, gt)\n",
    "        if is_halluc:\n",
    "            halluc_count += 1\n",
    "    \n",
    "    halluc_rate = halluc_count / len(test_prompts)\n",
    "    steering_results[condition_name] = {\n",
    "        'alphas': alphas,\n",
    "        'halluc_rate': halluc_rate,\n",
    "        'halluc_count': halluc_count,\n",
    "        'n_samples': len(test_prompts)\n",
    "    }\n",
    "    \n",
    "    print(f\"    Hallucination rate: {halluc_rate*100:.1f}% ({halluc_count}/{len(test_prompts)})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEERING RESULTS SUMMARY\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e5c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE STEERING RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "conditions = list(steering_results.keys())\n",
    "halluc_rates = [steering_results[c]['halluc_rate'] * 100 for c in conditions]\n",
    "\n",
    "colors = ['#95a5a6', '#3498db', '#e74c3c', '#9b59b6']\n",
    "bars = ax.bar(conditions, halluc_rates, color=colors, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax.set_ylabel('Hallucination Rate (%)', fontsize=12)\n",
    "ax.set_title('Multi-Layer Steering: Hallucination Rates', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# Add baseline reference line\n",
    "baseline_rate = steering_results['baseline']['halluc_rate'] * 100\n",
    "ax.axhline(baseline_rate, color='red', linestyle='--', alpha=0.7, label=f'Baseline: {baseline_rate:.1f}%')\n",
    "\n",
    "# Value labels\n",
    "for bar, rate in zip(bars, halluc_rates):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "            f'{rate:.1f}%', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.legend()\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('multilayer_steering.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Saved multilayer_steering.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d30cc19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚úÖ Multi-Layer Experiment Complete!\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Detection Comparison (Single vs Multi-Layer)\n",
    "\n",
    "| Configuration | Halluc AUC | Safety AUC | Notes |\n",
    "|---------------|------------|------------|-------|\n",
    "| Layer 12 only | - | - | Baseline (structural features) |\n",
    "| Layer 20 only | - | - | Deep layer (semantic features) |\n",
    "| Multi-Layer (12+20) | - | - | Combined (32k features) |\n",
    "\n",
    "*Fill in the values from your experimental results above.*\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Layer 20** may capture more abstract semantic concepts than Layer 12\n",
    "2. **Multi-layer concatenation** potentially provides complementary information\n",
    "3. **Steering at both layers** may show different effects than single-layer steering\n",
    "\n",
    "### Files Generated\n",
    "- `multilayer_comparison.png` - Detection performance comparison\n",
    "- `multilayer_steering.png` - Steering experiment results\n",
    "\n",
    "### For the Report\n",
    "Add this to your LaTeX Discussion section:\n",
    "\n",
    "```latex\n",
    "\\subsection{Multi-Layer Analysis}\n",
    "Following the suggestion to explore sparsity across different depths of the LLM, \n",
    "we compared SAE features from layers 12 (middle) and 20 (deep). Our hypothesis \n",
    "was that deeper layers encode more abstract semantic concepts that could improve \n",
    "detection and steering performance.\n",
    "\n",
    "Results show that [INSERT YOUR FINDINGS HERE]. When concatenating features from \n",
    "both layers (32,768 total features), we observed [INSERT OBSERVATION]. For \n",
    "steering, applying interventions at multiple layers simultaneously \n",
    "[IMPROVED/DID NOT SIGNIFICANTLY CHANGE] the hallucination rate compared to \n",
    "single-layer steering.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
