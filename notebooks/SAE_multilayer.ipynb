{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "00b4fdf9",
      "metadata": {
        "id": "00b4fdf9"
      },
      "source": [
        "# \ud83c\udf93 ECE685 Project 2: Multi-Layer SAE-Guided LLM Safety\n",
        "\n",
        "## Exploring Sparsity Across Multiple Layers via Sparse Autoencoders\n",
        "\n",
        "This notebook investigates how **Sparse Autoencoders (SAEs)** at **multiple layers** can improve interpretation and safety.\n",
        "\n",
        "### Multi-Layer Approach:\n",
        "Per TA Zihao's suggestion, we hook SAEs at **both layer 12 AND layer 20** to potentially boost performance:\n",
        "- **Layer 12** (mid-layer): Captures structural/syntactic patterns\n",
        "- **Layer 20** (late-layer): Captures semantic/contextual patterns\n",
        "\n",
        "### What We'll Do:\n",
        "1. **Load models**: Gemma-2-2B-IT + pretrained SAEs from Gemma Scope for layers 12 & 20\n",
        "2. **Capture activations**: Hook BOTH layers, encode with respective SAEs\n",
        "3. **Feature discovery**: Identify F\u207a/F\u207b features for each layer AND combined\n",
        "4. **Detection**: Compare L12-only vs L20-only vs Multi-layer classifiers\n",
        "5. **Steering**: Apply steering vectors at both layers simultaneously\n",
        "\n",
        "### Datasets:\n",
        "- **NQ-Open** (validation): Question answering \u2192 hallucination detection\n",
        "- **RealToxicityPrompts**: Toxic prompts \u2192 safety detection  \n",
        "- **Anthropic HH**: Harmless prompts \u2192 baseline for safety"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19cdbb3f",
      "metadata": {
        "id": "19cdbb3f"
      },
      "source": [
        "## \ud83d\udd27 Setup: Install Dependencies & Login to HuggingFace\n",
        "\n",
        "### \u26a0\ufe0f IMPORTANT: Follow These Steps Exactly!\n",
        "\n",
        "| Step | Action |\n",
        "|------|--------|\n",
        "| 1\ufe0f\u20e3 | **Run the installation cell below** (wait for it to finish, ~2-3 min) |\n",
        "| 2\ufe0f\u20e3 | **RESTART THE RUNTIME**: Click `Runtime \u2192 Restart runtime` |\n",
        "| 3\ufe0f\u20e3 | **After restart, SKIP the installation cell** (don't run it again!) |\n",
        "| 4\ufe0f\u20e3 | **Run the verification cell** to confirm everything works |\n",
        "| 5\ufe0f\u20e3 | Continue with the HuggingFace login and rest of notebook |\n",
        "\n",
        "> \ud83d\uded1 **Why restart?** Google Colab caches numpy in memory. After installing a new version, you MUST restart for Python to use the new version. If you skip the restart, you'll get a `numpy.dtype size changed` error.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebc68c14",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebc68c14",
        "outputId": "0edf7c72-6ddc-4869-8580-385e704e4fe0"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0583538",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "a0583538",
        "outputId": "a1aa50af-e801-4ddd-d1b6-805e96e72a4b"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 1: Install dependencies (RUN THIS FIRST, THEN RESTART!)\n",
        "# ============================================================================\n",
        "#\n",
        "# \u26a0\ufe0f  CRITICAL: After this cell finishes, you MUST:\n",
        "#     1. Click Runtime \u2192 Restart runtime (or Ctrl+M .)\n",
        "#     2. After restart, SKIP this cell\n",
        "#     3. Run the verification cell below\n",
        "#\n",
        "# ============================================================================\n",
        "\n",
        "# Uninstall any existing numpy first to avoid conflicts\n",
        "%pip uninstall -y numpy\n",
        "# Install compatible numpy version\n",
        "%pip install numpy==1.26.4\n",
        "# Install other dependencies\n",
        "%pip install -q transformers accelerate datasets torch pandas matplotlib scikit-learn tqdm\n",
        "# Install sae-lens (must be after numpy to avoid conflicts)\n",
        "%pip install -q sae-lens\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc7ec543",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc7ec543",
        "outputId": "8691dbff-6965-4e02-ad78-e24b0d646193"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 2: Verify installation (run AFTER restarting runtime)\n",
        "# ============================================================================\n",
        "# If you get a numpy error here, you forgot to restart the runtime!\n",
        "# Go back and follow the instructions in the installation cell.\n",
        "# ============================================================================\n",
        "\n",
        "import numpy as np\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "\n",
        "# Check if numpy version is compatible\n",
        "if np.__version__.startswith(\"2.\"):\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"\u274c ERROR: Wrong NumPy version detected!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\n   You have NumPy {np.__version__}, but need 1.26.x\")\n",
        "    print(\"\\n   This means you did NOT restart the runtime after installation.\")\n",
        "    print(\"\\n   FIX: Click Runtime \u2192 Restart runtime, then run THIS cell again\")\n",
        "    print(\"        (skip the installation cell after restart)\")\n",
        "    print(\"=\"*70)\n",
        "    raise RuntimeError(\"Please restart the runtime and try again!\")\n",
        "\n",
        "print(f\"\u2713 NumPy version: {np.__version__} (compatible)\")\n",
        "\n",
        "import sae_lens\n",
        "print(f\"\u2713 sae-lens version: {sae_lens.__version__}\")\n",
        "\n",
        "import torch\n",
        "print(f\"\u2713 PyTorch version: {torch.__version__}\")\n",
        "print(f\"\u2713 CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "import transformers\n",
        "from tqdm import tqdm\n",
        "print(f\"\u2713 transformers version: {transformers.__version__}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"\u2705 All packages loaded successfully!\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\nYou can now proceed with the rest of the notebook.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e3c203a",
      "metadata": {
        "id": "9e3c203a"
      },
      "outputs": [],
      "source": [
        "# Login to HuggingFace (required for Gemma access)\n",
        "# You need to accept the license at: https://huggingface.co/google/gemma-2b-it\n",
        "from huggingface_hub import login\n",
        "login(\"\")  # Enter your HuggingFace token when prompted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56f1e1a6",
      "metadata": {
        "id": "56f1e1a6"
      },
      "source": [
        "## \ud83d\udce6 Configuration & Helper Classes\n",
        "\n",
        "These classes wrap the models and provide a clean interface. All code is inline for Colab compatibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b84dda0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b84dda0c",
        "outputId": "e58fd93d-f4f8-4cdd-d244-b0d25e649b0d"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MULTI-LAYER CONFIGURATION\n",
        "# ============================================================================\n",
        "from dataclasses import dataclass, field\n",
        "from pathlib import Path\n",
        "from typing import Literal, Optional, Dict, Any, List\n",
        "import json\n",
        "\n",
        "@dataclass\n",
        "class MultiLayerConfig:\n",
        "    \"\"\"Multi-layer project configuration - hook SAEs at multiple layers\"\"\"\n",
        "    # Model settings - using Gemma 2 2B IT (hidden_size=2304) to match SAE\n",
        "    gemma_model_name: str = \"google/gemma-2-2b-it\"\n",
        "    toxicity_model_name: str = \"unitary/unbiased-toxic-roberta\"\n",
        "    dtype: str = \"bfloat16\"\n",
        "\n",
        "    # ========================================================================\n",
        "    # MULTI-LAYER SETTINGS: Hook SAEs at layers 12 AND 20\n",
        "    # Layer 12: mid-layer (structural patterns)\n",
        "    # Layer 20: late-layer (semantic patterns)\n",
        "    # ========================================================================\n",
        "    hook_layers: List[int] = field(default_factory=lambda: [12, 20])\n",
        "\n",
        "    # SAE settings per layer (Gemma Scope) - for Gemma 2 2B\n",
        "    sae_release: str = \"gemma-scope-2b-pt-res-canonical\"\n",
        "    sae_ids: Dict[int, str] = field(default_factory=lambda: {\n",
        "        12: \"layer_12/width_16k/canonical\",\n",
        "        20: \"layer_20/width_16k/canonical\"\n",
        "    })\n",
        "\n",
        "    # Data settings - USE ALL AVAILABLE DATA (matching Combined.ipynb)\n",
        "    # nq_sample_limit: int = 3610   # NQ-Open has ~3610 validation samples, use all\n",
        "    # hh_sample_limit: int = 8552   # Anthropic HH has ~8552 samples, use all\n",
        "    # rtp_sample_limit: int = 8552  # Sample this many from RTP to balance with HH\n",
        "    nq_sample_limit: int = 2   # NQ-Open has ~3610 validation samples, use all\n",
        "    hh_sample_limit: int = 2   # Anthropic HH has ~8552 samples, use all\n",
        "    rtp_sample_limit: int = 2  # Sample this many from RTP to balance with HH\n",
        "\n",
        "    # Batch sizes (matching Combined.ipynb)\n",
        "    data_batch_size: int = 64     # Batch size for data processing (larger for speed)\n",
        "    steering_batch_size: int = 16 # Batch size for steering experiments\n",
        "\n",
        "    # Steering experiment settings (matching Combined.ipynb)\n",
        "    steering_samples: int = 200   # 200 samples for steering experiments\n",
        "    steering_strength_ratios: list = field(default_factory=lambda: [0.0, 0.05, 0.1, 0.2, 0.3])\n",
        "\n",
        "    # Feature discovery settings\n",
        "    top_k_features: int = 100  # Top features per layer\n",
        "\n",
        "    # Experiment settings\n",
        "    device: str = \"cuda\"\n",
        "    seed: int = 42\n",
        "\n",
        "    @property\n",
        "    def n_features_per_layer(self) -> int:\n",
        "        \"\"\"16k features per layer\"\"\"\n",
        "        return 16384\n",
        "\n",
        "    @property\n",
        "    def total_features(self) -> int:\n",
        "        \"\"\"Total features when combining all layers\"\"\"\n",
        "        return self.n_features_per_layer * len(self.hook_layers)\n",
        "\n",
        "CONFIG = MultiLayerConfig()\n",
        "print(f\"\u2713 Multi-Layer Config loaded (COMPREHENSIVE MODE - GPU)\")\n",
        "print(f\"  Model: {CONFIG.gemma_model_name}\")\n",
        "print(f\"  Hook Layers: {CONFIG.hook_layers}\")\n",
        "print(f\"  SAE Release: {CONFIG.sae_release}\")\n",
        "print(f\"  SAE IDs:\")\n",
        "for layer, sae_id in CONFIG.sae_ids.items():\n",
        "    print(f\"    Layer {layer}: {sae_id}\")\n",
        "print(f\"  Features: {CONFIG.n_features_per_layer} per layer \u00d7 {len(CONFIG.hook_layers)} layers = {CONFIG.total_features} total\")\n",
        "print(f\"  Data limits: NQ={CONFIG.nq_sample_limit}, HH={CONFIG.hh_sample_limit}, RTP={CONFIG.rtp_sample_limit}\")\n",
        "print(f\"  Batch sizes: data={CONFIG.data_batch_size}, steering={CONFIG.steering_batch_size}\")\n",
        "print(f\"  Steering samples: {CONFIG.steering_samples}\")\n",
        "print(f\"  Steering ratios (of \u2016h\u2016): {CONFIG.steering_strength_ratios}\")\n",
        "n_modes = 2  # layer_12, layer_20\n",
        "print(f\"  Total steering generations: {CONFIG.steering_samples} \u00d7 {len(CONFIG.steering_strength_ratios)} \u00d7 {n_modes} modes \u00d7 2 tasks = {CONFIG.steering_samples * len(CONFIG.steering_strength_ratios) * n_modes * 2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f28e22dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f28e22dd",
        "outputId": "b17e9c35-5acb-48f8-cc2a-6ba6755b6f31"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MULTI-LAYER SPARSE AUTOENCODER WRAPPER\n",
        "# ============================================================================\n",
        "import torch\n",
        "from sae_lens import SAE\n",
        "import numpy as np\n",
        "\n",
        "@dataclass\n",
        "class SingleLayerSAE:\n",
        "    \"\"\"Wrapper for a single layer's SAE with encode/decode methods\"\"\"\n",
        "    layer_idx: int\n",
        "    encoder_weight: torch.Tensor  # [n_features, d_model]\n",
        "    decoder_weight: torch.Tensor  # [n_features, d_model]\n",
        "    bias: torch.Tensor            # [n_features]\n",
        "    n_features: int\n",
        "    device: str = \"cpu\"\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def encode(self, residual: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Encode residual activations to sparse SAE codes\"\"\"\n",
        "        original_shape = residual.shape\n",
        "        residual = residual.float().cpu()\n",
        "\n",
        "        if residual.dim() == 3:\n",
        "            residual = residual.reshape(-1, residual.size(-1))\n",
        "\n",
        "        projected = torch.nn.functional.linear(residual, self.encoder_weight, self.bias)\n",
        "        codes = torch.nn.functional.relu(projected)\n",
        "\n",
        "        if len(original_shape) == 3:\n",
        "            codes = codes.reshape(original_shape[0], original_shape[1], -1)\n",
        "\n",
        "        return codes\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def decode(self, codes: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Decode SAE codes back to residual space\"\"\"\n",
        "        return torch.nn.functional.linear(codes, self.decoder_weight.t())\n",
        "\n",
        "\n",
        "class MultiLayerSAE:\n",
        "    \"\"\"\n",
        "    Multi-layer SAE manager that loads and manages SAEs for multiple layers.\n",
        "    Supports encoding from any single layer or all layers concatenated.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.saes: Dict[int, SingleLayerSAE] = {}\n",
        "        self.layer_order: List[int] = []\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, config: MultiLayerConfig) -> \"MultiLayerSAE\":\n",
        "        \"\"\"Load pretrained SAEs for all configured layers\"\"\"\n",
        "        instance = cls()\n",
        "\n",
        "        print(f\"Loading Multi-Layer SAEs from Gemma Scope...\")\n",
        "        print(f\"  Release: {config.sae_release}\")\n",
        "        print(f\"  Layers: {config.hook_layers}\")\n",
        "\n",
        "        for layer_idx in config.hook_layers:\n",
        "            sae_id = config.sae_ids[layer_idx]\n",
        "            print(f\"\\n  Loading SAE for layer {layer_idx}: {sae_id}...\")\n",
        "\n",
        "            sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
        "                release=config.sae_release,\n",
        "                sae_id=sae_id,\n",
        "                device=\"cpu\",\n",
        "            )\n",
        "\n",
        "            print(f\"    \u2713 Features: {sae.cfg.d_sae}, Hidden size: {sae.cfg.d_in}\")\n",
        "\n",
        "            # W_enc: (d_in, d_sae) \u2192 transpose to (d_sae, d_in)\n",
        "            encoder_w = sae.W_enc.detach().cpu().t()\n",
        "            decoder_w = sae.W_dec.detach().cpu()\n",
        "\n",
        "            single_sae = SingleLayerSAE(\n",
        "                layer_idx=layer_idx,\n",
        "                encoder_weight=encoder_w,\n",
        "                decoder_weight=decoder_w,\n",
        "                bias=sae.b_enc.detach().cpu() if hasattr(sae, \"b_enc\") else torch.zeros(sae.cfg.d_sae),\n",
        "                n_features=sae.cfg.d_sae,\n",
        "            )\n",
        "            instance.saes[layer_idx] = single_sae\n",
        "            instance.layer_order.append(layer_idx)\n",
        "\n",
        "        print(f\"\\n\u2713 Multi-Layer SAE loaded!\")\n",
        "        print(f\"  Total layers: {len(instance.saes)}\")\n",
        "        print(f\"  Features per layer: {config.n_features_per_layer}\")\n",
        "        print(f\"  Total features (concatenated): {config.total_features}\")\n",
        "\n",
        "        return instance\n",
        "\n",
        "    def encode_single_layer(self, residual: torch.Tensor, layer_idx: int) -> torch.Tensor:\n",
        "        \"\"\"Encode residual from a single layer\"\"\"\n",
        "        if layer_idx not in self.saes:\n",
        "            raise ValueError(f\"No SAE loaded for layer {layer_idx}\")\n",
        "        return self.saes[layer_idx].encode(residual)\n",
        "\n",
        "    def encode_all_layers(self, residuals: Dict[int, torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Encode residuals from all layers and concatenate.\n",
        "\n",
        "        Args:\n",
        "            residuals: Dict mapping layer_idx \u2192 residual tensor\n",
        "\n",
        "        Returns:\n",
        "            Concatenated codes [n_features_layer1, n_features_layer2, ...]\n",
        "        \"\"\"\n",
        "        codes_list = []\n",
        "        for layer_idx in self.layer_order:\n",
        "            if layer_idx not in residuals:\n",
        "                raise ValueError(f\"Missing residual for layer {layer_idx}\")\n",
        "            codes = self.saes[layer_idx].encode(residuals[layer_idx])\n",
        "            codes_list.append(codes)\n",
        "\n",
        "        # Concatenate along feature dimension\n",
        "        return torch.cat(codes_list, dim=-1)\n",
        "\n",
        "    def get_codes_dict(self, residuals: Dict[int, torch.Tensor]) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Get codes for each layer individually AND concatenated.\n",
        "\n",
        "        Returns:\n",
        "            Dict with keys: 'layer_12', 'layer_20', 'multi_layer'\n",
        "        \"\"\"\n",
        "        result = {}\n",
        "        codes_list = []\n",
        "\n",
        "        for layer_idx in self.layer_order:\n",
        "            if layer_idx not in residuals:\n",
        "                raise ValueError(f\"Missing residual for layer {layer_idx}\")\n",
        "            codes = self.saes[layer_idx].encode(residuals[layer_idx])\n",
        "            result[f'layer_{layer_idx}'] = codes.squeeze(0).numpy()\n",
        "            codes_list.append(codes)\n",
        "\n",
        "        # Concatenate for multi-layer\n",
        "        multi_codes = torch.cat(codes_list, dim=-1)\n",
        "        result['multi_layer'] = multi_codes.squeeze(0).numpy()\n",
        "\n",
        "        return result\n",
        "\n",
        "    def build_steering_vector(self, layer_idx: int, f_plus_ids: list, f_minus_ids: list,\n",
        "                               plus_weight: float = -1.0, minus_weight: float = 1.0) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Build steering vector for a specific layer from F\u207a and F\u207b features.\n",
        "        \"\"\"\n",
        "        if layer_idx not in self.saes:\n",
        "            raise ValueError(f\"No SAE loaded for layer {layer_idx}\")\n",
        "\n",
        "        sae = self.saes[layer_idx]\n",
        "        d_in = sae.decoder_weight.shape[1]\n",
        "        direction = torch.zeros(d_in)\n",
        "\n",
        "        for idx in f_plus_ids:\n",
        "            if idx < sae.n_features:\n",
        "                direction += plus_weight * sae.decoder_weight[idx]\n",
        "        for idx in f_minus_ids:\n",
        "            if idx < sae.n_features:\n",
        "                direction += minus_weight * sae.decoder_weight[idx]\n",
        "\n",
        "        if direction.norm() > 0:\n",
        "            direction = direction / direction.norm()\n",
        "\n",
        "        return direction\n",
        "\n",
        "print(\"\u2713 MultiLayerSAE class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9974d68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9974d68",
        "outputId": "d2ba11a7-f656-4716-a3eb-f52661120644"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MULTI-LAYER GEMMA INTERFACE WITH ACTIVATION HOOKS\n",
        "# ============================================================================\n",
        "from contextlib import contextmanager\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "@dataclass\n",
        "class MultiLayerResidualCapture:\n",
        "    \"\"\"Captures residual stream activations from MULTIPLE layers\"\"\"\n",
        "    layer_indices: List[int]\n",
        "    residuals: Dict[int, torch.Tensor] = field(default_factory=dict)\n",
        "\n",
        "    def create_hook(self, layer_idx: int):\n",
        "        \"\"\"Create a hook function for a specific layer\"\"\"\n",
        "        def hook_fn(module, inputs, output):\n",
        "            if isinstance(output, tuple):\n",
        "                output = output[0]\n",
        "            self.residuals[layer_idx] = output.detach()\n",
        "        return hook_fn\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"Clear captured residuals\"\"\"\n",
        "        self.residuals = {}\n",
        "\n",
        "\n",
        "class MultiLayerGemmaInterface:\n",
        "    \"\"\"\n",
        "    Wrapper for Gemma with multi-layer activation capture and steering hooks.\n",
        "    Captures residuals from ALL configured layers simultaneously.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = None, config: MultiLayerConfig = None):\n",
        "        self.config = config or CONFIG\n",
        "        model_id = model_name or self.config.gemma_model_name\n",
        "        dtype = getattr(torch, self.config.dtype)\n",
        "\n",
        "        print(f\"Loading {model_id}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=dtype,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "\n",
        "        self.capture = MultiLayerResidualCapture(layer_indices=self.config.hook_layers)\n",
        "        self._activation_handles: Dict[int, Any] = {}\n",
        "        self._steering_handles: Dict[int, Any] = {}\n",
        "\n",
        "        print(f\"\u2713 Gemma loaded on {self.model.device}\")\n",
        "        print(f\"  Hooking layers: {self.config.hook_layers}\")\n",
        "\n",
        "    def register_capture_hooks(self):\n",
        "        \"\"\"Register capture hooks for ALL configured layers\"\"\"\n",
        "        for layer_idx in self.config.hook_layers:\n",
        "            if layer_idx not in self._activation_handles:\n",
        "                block = self.model.model.layers[layer_idx]\n",
        "                hook_fn = self.capture.create_hook(layer_idx)\n",
        "                self._activation_handles[layer_idx] = block.register_forward_hook(hook_fn)\n",
        "\n",
        "    def register_steering_hook(self, layer_idx: int, steering_vector: torch.Tensor, strength: float):\n",
        "        \"\"\"Register steering hook for a specific layer\"\"\"\n",
        "        def steering_fn(module, inputs, output):\n",
        "            is_tuple = isinstance(output, tuple)\n",
        "            hidden_states = output[0] if is_tuple else output\n",
        "\n",
        "            shifted = hidden_states.clone()\n",
        "            steer = steering_vector.to(shifted.device).to(shifted.dtype) * strength\n",
        "            shifted[:, -1, :] += steer\n",
        "\n",
        "            if is_tuple:\n",
        "                return (shifted,) + output[1:] if len(output) > 1 else (shifted,)\n",
        "            return shifted\n",
        "\n",
        "        block = self.model.model.layers[layer_idx]\n",
        "        self._steering_handles[layer_idx] = block.register_forward_hook(steering_fn)\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        \"\"\"Remove all registered hooks\"\"\"\n",
        "        for handle in self._activation_handles.values():\n",
        "            if handle is not None:\n",
        "                handle.remove()\n",
        "        for handle in self._steering_handles.values():\n",
        "            if handle is not None:\n",
        "                handle.remove()\n",
        "        self._activation_handles = {}\n",
        "        self._steering_handles = {}\n",
        "        self.capture.clear()\n",
        "\n",
        "    @contextmanager\n",
        "    def capture_residuals(self, steering_vectors: Optional[Dict[int, torch.Tensor]] = None,\n",
        "                          strengths: Optional[Dict[int, float]] = None):\n",
        "        \"\"\"\n",
        "        Context manager to capture residuals from all layers.\n",
        "        Optionally applies steering at specified layers.\n",
        "        \"\"\"\n",
        "        self.capture.clear()\n",
        "        self.register_capture_hooks()\n",
        "\n",
        "        if steering_vectors and strengths:\n",
        "            for layer_idx, vec in steering_vectors.items():\n",
        "                if layer_idx in strengths and strengths[layer_idx] != 0.0:\n",
        "                    self.register_steering_hook(layer_idx, vec, strengths[layer_idx])\n",
        "        try:\n",
        "            yield self.capture\n",
        "        finally:\n",
        "            self.remove_hooks()\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate(self, prompt: str, max_new_tokens: int = 50,\n",
        "                 steering_vectors: Optional[Dict[int, torch.Tensor]] = None,\n",
        "                 strengths: Optional[Dict[int, float]] = None) -> dict:\n",
        "        \"\"\"Generate text with optional multi-layer steering and residual capture\"\"\"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        with self.capture_residuals(steering_vectors, strengths):\n",
        "            output_ids = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=False,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Get last token residuals for each layer\n",
        "        residuals = {}\n",
        "        for layer_idx, res in self.capture.residuals.items():\n",
        "            if res is not None:\n",
        "                residuals[layer_idx] = res[:, -1, :].detach().cpu()\n",
        "\n",
        "        return {\"text\": text, \"residuals\": residuals}\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate_batch(self, prompts: list, max_new_tokens: int = 50,\n",
        "                       steering_vectors: Optional[Dict[int, torch.Tensor]] = None,\n",
        "                       strengths: Optional[Dict[int, float]] = None) -> list:\n",
        "        \"\"\"\n",
        "        Batch generate text with optional multi-layer steering and residual capture.\n",
        "        \"\"\"\n",
        "        if not prompts:\n",
        "            return []\n",
        "\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        inputs = self.tokenizer(\n",
        "            prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(self.model.device)\n",
        "\n",
        "        # Store residuals for each layer\n",
        "        batch_residuals: Dict[int, list] = {layer: [] for layer in self.config.hook_layers}\n",
        "\n",
        "        def create_batch_hook(layer_idx):\n",
        "            def hook_fn(module, inp, output):\n",
        "                hidden = output[0] if isinstance(output, tuple) else output\n",
        "                batch_residuals[layer_idx].append(hidden.detach().cpu())\n",
        "            return hook_fn\n",
        "\n",
        "        # Register hooks\n",
        "        handles = []\n",
        "        for layer_idx in self.config.hook_layers:\n",
        "            block = self.model.model.layers[layer_idx]\n",
        "            handle = block.register_forward_hook(create_batch_hook(layer_idx))\n",
        "            handles.append(handle)\n",
        "\n",
        "        try:\n",
        "            output_ids = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=False,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "            )\n",
        "        finally:\n",
        "            for handle in handles:\n",
        "                handle.remove()\n",
        "\n",
        "        texts = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "        results = []\n",
        "        for i, text in enumerate(texts):\n",
        "            residuals = {}\n",
        "            for layer_idx in self.config.hook_layers:\n",
        "                if batch_residuals[layer_idx]:\n",
        "                    last_hidden = batch_residuals[layer_idx][0]\n",
        "                    if i < last_hidden.shape[0]:\n",
        "                        seq_len = (inputs['attention_mask'][i] == 1).sum().item()\n",
        "                        residuals[layer_idx] = last_hidden[i, seq_len-1, :].unsqueeze(0)\n",
        "\n",
        "            results.append({\n",
        "                \"text\": text,\n",
        "                \"residuals\": residuals\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def forward_batch(self, prompts: list) -> list:\n",
        "        \"\"\"\n",
        "        Run forward pass only (no generation) to capture multi-layer residuals.\n",
        "        Much faster for activation capture when you don't need generated text.\n",
        "        \"\"\"\n",
        "        if not prompts:\n",
        "            return []\n",
        "\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        inputs = self.tokenizer(\n",
        "            prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(self.model.device)\n",
        "\n",
        "        self.register_capture_hooks()\n",
        "        try:\n",
        "            _ = self.model(**inputs, output_hidden_states=False)\n",
        "\n",
        "            results = []\n",
        "            for i in range(len(prompts)):\n",
        "                residuals = {}\n",
        "                seq_len = (inputs['attention_mask'][i] == 1).sum().item()\n",
        "                for layer_idx in self.config.hook_layers:\n",
        "                    if layer_idx in self.capture.residuals:\n",
        "                        residuals[layer_idx] = self.capture.residuals[layer_idx][i, seq_len-1, :].unsqueeze(0).cpu()\n",
        "                results.append(residuals)\n",
        "        finally:\n",
        "            self.remove_hooks()\n",
        "\n",
        "        return results\n",
        "\n",
        "print(\"\u2713 MultiLayerGemmaInterface class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90a3651e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90a3651e",
        "outputId": "f2ad4a4b-d20a-4992-f269-57b673b57c4e"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TOXICITY CLASSIFIER WRAPPER\n",
        "# ============================================================================\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "@dataclass\n",
        "class ToxicityScore:\n",
        "    probability: float\n",
        "    label: int  # 1 = toxic, 0 = safe\n",
        "\n",
        "\n",
        "class ToxicityWrapper:\n",
        "    \"\"\"Wrapper for toxicity classification model\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = None):\n",
        "        model_id = model_name or CONFIG.toxicity_model_name\n",
        "        print(f\"Loading toxicity classifier: {model_id}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
        "        self.model.eval()\n",
        "        print(\"\u2713 Toxicity classifier loaded\")\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def score(self, text: str, threshold: float = 0.5) -> ToxicityScore:\n",
        "        \"\"\"Score text for toxicity\"\"\"\n",
        "        if not text or len(text.strip()) == 0:\n",
        "            return ToxicityScore(probability=0.0, label=0)\n",
        "\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "        logits = self.model(**inputs).logits\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        prob = probs[0, 1].item()  # Probability of toxic class\n",
        "        return ToxicityScore(probability=prob, label=int(prob >= threshold))\n",
        "\n",
        "print(\"\u2713 ToxicityWrapper class defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cca116a",
      "metadata": {
        "id": "9cca116a"
      },
      "source": [
        "---\n",
        "\n",
        "# \ud83d\ude80 Part 1: Load Models & Prepare Data\n",
        "\n",
        "Now we load all three models:\n",
        "1. **Gemma-2B-IT**: The base LLM for text generation\n",
        "2. **SAE (Gemma Scope)**: Pretrained sparse autoencoder for layer 12\n",
        "3. **Toxicity Classifier**: For labeling safe/unsafe completions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56842526",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ec80b62ecade4194b3eb2d9c448fc274",
            "99d5f0772780466ab3eccc218fbc5368",
            "221d6be3100a44acb77c1653b7ead970",
            "396796106d3a46de948cb1b67ce699e8",
            "59f165b020e942e8b365589900367a19",
            "bace7f9bf55941e8ada35f426e815706",
            "9de23433a4064addbb1327a9ebf33042",
            "dafc546016e9452db5ee35397535eec3",
            "7ac778f2353d4f578e6a2dc1ebaabbe4",
            "d2b34aacb6cd4e9290a47fc39e9b510a",
            "84249032829049a9b9aedf39b182dc56",
            "6896d4bffe714c459c23f2d00188064a",
            "cfa48db2003f474e80104fc9b8b36c74",
            "4b3c590885314a3f914cb037a0dda10e",
            "1e0c62e647154a3b843ac430f229c579",
            "cc5d9ccbfc584c989c4a2dbbe02dafba",
            "7723a11c4ec542fea35e6ef35915a436",
            "84b3d730b3864406a9aa39301c7788d7",
            "cc625502837b462db0a9d20f933ffe31",
            "86edb0d2b48f45fd8f3f37f8d91bc534",
            "15c9dd2ea2134f49968a90ac6fd5b614",
            "7af5dd2258ce42bebe01c421bd05b6b7",
            "2d5a6b98ad904cf3a2da909a3d224f04",
            "43a757855de746e091758d3d5479381f",
            "1e9e04f23feb4bf99a2f058377d7aa2f",
            "e6f4cb4a24f14975bb0703d99f0070c8",
            "8bf0930754df4337af327ff50a3b0277",
            "7ecaf37725f540e689e85d4eb1c4540a",
            "456a58325e4b4a888574b1dd23ef0bf7",
            "7cf6de3d7c9c4ec182888318e3e2a1bc",
            "7b674fabca054c8db3629cf1cf78aa5a",
            "05ca2446471e4f1380b42b58289e57ba",
            "e4d80709e0304ee7a665026e03614dae",
            "5a0b2eadd4e8454a93a99a141c3f1a74",
            "d7757762077747b3b85bb2fb0e739131",
            "c255cdcdcca141b3add14857474c2e90",
            "245ac20660424f678eb0f6a33471da7d",
            "abc14621d91b46c089d3831e2c67862b",
            "5b14d259c04f4353b838ea444dd5fbd3",
            "59cfce0bdb224ec985145c7baedd53d6",
            "d94950e7627a4d12b462895338a06f3d",
            "9a0a155822214618afca3aedeb2acfe0",
            "011c232b05834d4ea989d5b07bf1c87c",
            "20a926bba2c3436d992d2dc2a8c732f4",
            "a02ed48486c94bfb8a388e905bb7e0b5",
            "ab2009ea53e647ceb9402a656e2b8a17",
            "a8337bed339549e6aca52077e26aea3c",
            "0b603089fe724179bc0be934b8854986",
            "e8facd04bde9408fbad1d1c7546373ea",
            "b03880bdd73c43baac096e13be4c4d9f",
            "e2f60129cb0e4181a63df01efd46f206",
            "705f0ed65aa345e0a9200d78afcaf5ad",
            "109bddf240c34e558b247e7012f04df6",
            "49a21adb8ff14cdf84b97efd6a36ed5b",
            "c3f595988c5d4fd4901b395cadf58128",
            "f189d1f2ac974d16b4eef7f80486f798",
            "4dedad4db23349928b14fa1e4e030d1a",
            "17ac67c9c68e4021852fcfb682355a9e",
            "b3125956f6fc43449c3b0c3dbc738724",
            "88cf3871abea4c17b3ca814dde5acaaa",
            "999eea84edd4488ebbb49cae4c2f354e",
            "8caa7b5899f9493297567cdd42e247ff",
            "c73447f7a0e648b0ba4dee7725f63177",
            "c4147ac0375248279c70847de4815c08",
            "f06a5c920484468494c9ddf217159255",
            "bbb6f357cc7c4944b89270d90f68b272",
            "2bac814debf8445caceaa426195d9a78",
            "3f4e4f6c16b4494f992a81ef9a2e4951",
            "636940b7f0a643e4bcb9f8b6ddd74fa2",
            "6c01e7dfdeef4bf894606f023293529e",
            "628ba01aa85647c5b3250b7cf7cf791e",
            "34410a1d2ee541c693f6e1876c521320",
            "ee43b19f95e144a990e8f5f7c703f39c",
            "336a609b5f59406fa49d43dd3aca226a",
            "9a032da94955479c84e01c937e19d420",
            "22ecf04efa0442ba92964d0c2e75371b",
            "bf4a9ae1ac7f41e2bbf78f993c109646",
            "17930069a0ba4d81a58ef53979db94c6",
            "e202cd38064c499390b4914c0460a0d5",
            "db43df56866741559ae24212ef5d1c3b",
            "4c8bce34bdff41089a44483f5c3c16a9",
            "26afbee8f7444c0c95277d875dd16cb4",
            "d89a86d477e741889c6d7f8ae961e1de",
            "b02fe21157674ad493f261b761514762",
            "b6042f213628470395a57f42a77ec116",
            "2dd6c14227ae451cac9a231538bcb853",
            "7fd93de6aa3d4fd58dd9b1caf8bca3ef",
            "07379ca029a94d9aabc98ecea0c1bff2",
            "7c763bce12964e5c886b762c75b4fba2",
            "4344d9a540214db5b1cad25d937e3fad",
            "e7366c915fcb41778cc54eaac8c62cfc",
            "f73d1f3aba944aa4974781916b9ad929",
            "063ac0d7ad404e0e809ee1d56686491b",
            "60c2f63fd7d8422a82a8949793922ece",
            "1d4cb21b90d4432083b2e8e7d7e13bad",
            "892f340210aa4fc0baaf6d1b469791e0",
            "5a332c89512343028d35322ce628d84e",
            "7339f50b8c024c409b5562bbb7af92b8",
            "87310af4bdb147199532f332e6ebc667",
            "42e0449771b74bb681a716329d153cfb",
            "8529a235c5094666936ed2155bd57523",
            "def54d8e5e5448bca8b16985e1db7478",
            "68ec8f8d4d7740e680083839bc717e37",
            "3c83344b6e714b4faef52f847258b10c",
            "9dfb8146712a4cf8a9cbd94443201f15",
            "8b6dc8c82b454db98553217376765b97",
            "7228c32c91a048299f7c4ab944436885",
            "6f20ad3fb2b34ce7b04e502a0d388307",
            "244191fbfc9245508cba6fda1aeb6289",
            "b55bcf8ff0e84e93b3239f34a8047952",
            "1a12ab6bbc794e7bab5ba778eda87ad2",
            "09ad58afda5148b8a81b21522fe83d9e",
            "e8886ab05600453693f936d10afb20be",
            "ff75d706f4c24efcb612cd5788afdc3d",
            "391e6286bbf545a9be94e9624e06eee6",
            "27f4dba45a194e399f94f989d067db58",
            "5f9558ebd30f415e8665545d3ecbcc07",
            "c3abce3a59684c20b0d3e2195343f881",
            "dd051580a7cc428cba1d13ef81ea9cd0",
            "8fe7727e97084f7b998d1f76975bbf82",
            "200df479df3746b9a1ea326ff69af6a2",
            "70b4e8c996cf44bb84721fe11283fbfe",
            "45cb5477600a42efa2fe105a89166b9c",
            "9f67b55d2f5b41d7b5bb5851363fd932",
            "f44ec6228e8b4cca91a8bfc437d96a1b",
            "bf04dfe360c94f07abfb192bb7cf65cb",
            "abb2153f5fa64f6fa318fbea3bd26aca",
            "cc5e46b4308142ab8e606dad9d949969",
            "cf07593f3a3441a5a6f216cfe464054d",
            "2f5160c0f87942f7881984db8574760e",
            "6cae31be20474742ad043d6dc9df42e3",
            "4e032a4727ed44d08906891f2fcf051b",
            "37670a456d1a4bb28e387297fc892844",
            "8b1e156980794678a65cf492fa971ff5",
            "5b04c7af49c741e4807940533a187623",
            "18c97c81d40642bb83816a962f01d9c0",
            "f1064f5cef024147accb145f4ea6e68b",
            "92ee99b0fa13437794dbc4f92a8a33bc",
            "e4e26ad6f9aa4286979e75fc2fc6ffb0",
            "7328100698684de9be93513d1ed01676",
            "231eb4c6f7ca4751aa9d5c6d4acdf581",
            "9a30df1be9f54f9fa597a0713e97a5ad",
            "dd9d065dd9684f87a87aeda408424306",
            "9ad9c2c7d96b406f9b9ff21954b80130",
            "cadf12a084444b9a8371c2e6ed0e540e",
            "42ec0518107b4fdc9f5f575c9b2c799f",
            "cc07a653e9aa4624a63edf94425c861e",
            "05562f8968e54683a56a548c9c9a95b0",
            "c88e89fae20647ecbc923519fe4ae590",
            "0f770665e0cb4a29b56deff1ef4aa767",
            "aec6022149c94e008dc7e862e3e90a3a",
            "f231021e09204689959c20968be73be0",
            "45b842432e604cde821bd68d7c4d3e97",
            "c421421b344b412fb0ebb0912d33beec",
            "1cb72087867144b6af73f3aca5e0b21e",
            "981a073b4b494be7af4db0b28e5a46db",
            "873b9407d40a4b13aabe3100e872e0e5",
            "aa2f5e220ea343229a6926b8cd14be82",
            "4fdad01c42b7428ab98860c7f3f096c1",
            "a9a578c743084d00b0f58060ccbcee7e",
            "c33205149b504abea6b234b938a883c6",
            "36f56dc959bc495ea4347219bb975733",
            "c589aaf7bc8d43c08bcf1f4fcc118880",
            "22c4530f35574ddcb612d6eab177f325",
            "5a668bb161a94b41996e00ba55dae31a",
            "3d78faf8ca2e4937add17cf4b9fab4a8",
            "91d4c484cef641d6bc89b65c651c2a3b",
            "aecc93f79eff49128e98a2d2ba2e4c75",
            "737a16ee1de3437892ba3ba6ee23a81b",
            "fff3ca9a785b431f89e0bc7e3b4955d2",
            "3d4ea120cc9b4b0f8e4369522972000d",
            "32c35c3b8cf941b5832e549cc285e4d6",
            "89ac2a2cab1049f8bffa8f6bee9d2e7b",
            "d18c76f58cbc498a995a87c81510b1b1",
            "830ec24124d34c3798798765d94abe51",
            "7b41852b60ea48c7a3081991203d7a13",
            "e84dc81b18644cb3a35b3f841b43e8bf",
            "bbb8f2c1508c455982e2aedba73c16c5",
            "9da83e444e564507b7ad51a550506f9a",
            "b19135181ee24412b86ec1345797aaf9",
            "60d4e10edd424efcae3bf3b6b50564fb",
            "9f01a596960d4cb4adbf930dfe4ce697",
            "8e827b1287c4473f965b81169eb5278a",
            "1aeb9a402ca241e289d3e7a574dcab42",
            "f1b0c5cc9ec44083a16c531dc0d74b11",
            "5c1424d6b3fe4f209ec1229a1c62c331",
            "7ede2d8c3f844fa1ac2036ae617ce637",
            "b9538a2c88c34f47bca451935d4c64ec",
            "a9fa9b93eb5048b296f8029f79c2d9d2",
            "162070d1a7d44166b0424e742f8b7e57",
            "73e5e1a2957243af842f75ec12a05d23",
            "ac1828e874f349e7acd979b04e244eeb",
            "31fafc7d781e4fe29241095bea8b1099",
            "4ff47b855be642908d2020fdef71492c",
            "b9c0375651cb40c094e85bd89e767962",
            "18db617da1234d27b0a025e11a687e2d",
            "ff667ef48c4b4b2cbd355580f21d55be",
            "676b1b00c9a5445889177c4b094b87ad",
            "77eed7f44a7049c19824becc9f761d41",
            "dd38a0dd88544f8f8cec3730077aaa19",
            "2e1f0dd1e7bc48ce931da62141ec5939",
            "aa35e4589166495f8ed2453afd57d75b",
            "62f6493020a044bb8c957828ab7889bb",
            "a83c40c12f764d3db371818947b14f55",
            "8cd63815fb8345bda341e63f71d7c00c",
            "9680d21b85cd484ebdd91f31838d8001",
            "40cece2563b54ebeb96401920603addb",
            "440a58a8a9374f1a96d09016d05252d8",
            "e899c7daaf0b4e6abb6ce23b9515c802"
          ]
        },
        "id": "56842526",
        "outputId": "e1a2e19e-b5b5-4d0d-b432-a9867a1221b1"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LOAD ALL MODELS (MULTI-LAYER)\n",
        "# ============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"LOADING MODELS (MULTI-LAYER)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Load Gemma with multi-layer hooks\n",
        "gemma = MultiLayerGemmaInterface(config=CONFIG)\n",
        "hidden_size = gemma.model.config.hidden_size\n",
        "print(f\"  Hidden size: {hidden_size}\")\n",
        "\n",
        "# 2. Load Multi-Layer SAE (both layer 12 and layer 20)\n",
        "print()\n",
        "sae = MultiLayerSAE.load(CONFIG)\n",
        "\n",
        "# 3. Load toxicity classifier\n",
        "print()\n",
        "tox = ToxicityWrapper()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"\u2713 ALL MODELS LOADED SUCCESSFULLY!\")\n",
        "print(\"  - Gemma-2-2B-IT with hooks at layers\", CONFIG.hook_layers)\n",
        "print(\"  - Multi-Layer SAE (32k features total)\")\n",
        "print(\"  - Toxicity classifier\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68361bfe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466,
          "referenced_widgets": [
            "93bea2bb88d3477e8349b8cd0256e5f9",
            "cc5d8e74a5234baa936fe0b7ec3d9323",
            "cfe32e39695745f78d2afa26ef0769b3",
            "843c2e8968874464ab67418e37bfa1d5",
            "d00e3567e1de402d8f0ce8826d05aac5",
            "1c326073319c43bc9bcddb1cdf82855d",
            "298bcedb290c43a48ad807ced611b52c",
            "f1cdac060d944dfc959f1ace0361a13b",
            "ee82fee4df1e440e8f918d9fc6273171",
            "0d58d8f833834bc89989e88228e2c6dd",
            "54b0d278157143eca2605f5fafd31bf4"
          ]
        },
        "id": "68361bfe",
        "outputId": "32f0f873-a440-4cb4-97a4-03a3d6796aec"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CALIBRATE STEERING STRENGTH (MULTI-LAYER)\n",
        "# ============================================================================\n",
        "# Measure typical residual norm at each layer to properly calibrate steering\n",
        "\n",
        "def calibrate_multilayer_steering(gemma, sample_prompts: list, n_samples: int = 50):\n",
        "    \"\"\"\n",
        "    Measure typical residual norm at each layer to calibrate steering strength.\n",
        "    Returns mean norms per layer and computed alpha values.\n",
        "    \"\"\"\n",
        "    print(\"Calibrating multi-layer steering strengths...\")\n",
        "\n",
        "    norms_per_layer = {layer: [] for layer in CONFIG.hook_layers}\n",
        "\n",
        "    for i, prompt in enumerate(tqdm(sample_prompts[:n_samples], desc=\"Measuring residual norms\")):\n",
        "        result = gemma.generate(prompt, max_new_tokens=1)\n",
        "        for layer_idx, res in result['residuals'].items():\n",
        "            if res is not None:\n",
        "                norm = res.norm().item()\n",
        "                norms_per_layer[layer_idx].append(norm)\n",
        "\n",
        "    mean_norms = {}\n",
        "    alphas_per_layer = {}\n",
        "\n",
        "    print(f\"\\n  Residual stream statistics per layer:\")\n",
        "    for layer_idx in CONFIG.hook_layers:\n",
        "        norms = norms_per_layer[layer_idx]\n",
        "        mean_norm = np.mean(norms)\n",
        "        std_norm = np.std(norms)\n",
        "        mean_norms[layer_idx] = mean_norm\n",
        "\n",
        "        # Compute alpha values for this layer\n",
        "        alphas = [ratio * mean_norm for ratio in CONFIG.steering_strength_ratios]\n",
        "        alphas_per_layer[layer_idx] = alphas\n",
        "\n",
        "        print(f\"    Layer {layer_idx}: Mean \u2016h\u2016 = {mean_norm:.2f} \u00b1 {std_norm:.2f}\")\n",
        "        print(f\"      \u03b1 values: {[f'{a:.2f}' for a in alphas]}\")\n",
        "\n",
        "    return mean_norms, alphas_per_layer\n",
        "\n",
        "# Use simple calibration prompts (50 total, matching Combined.ipynb)\n",
        "calibration_prompts = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"How does photosynthesis work?\",\n",
        "    \"Explain quantum computing.\",\n",
        "    \"What causes climate change?\",\n",
        "    \"Who wrote Romeo and Juliet?\",\n",
        "    \"What is machine learning?\",\n",
        "    \"How do vaccines work?\",\n",
        "    \"What is the speed of light?\",\n",
        "    \"Explain the theory of relativity.\",\n",
        "    \"What is DNA?\",\n",
        "] * 5  # 50 prompts total\n",
        "\n",
        "MEAN_NORMS, ALPHAS_PER_LAYER = calibrate_multilayer_steering(gemma, calibration_prompts, n_samples=50)\n",
        "\n",
        "# Use average of layer norms for unified steering strength\n",
        "MEAN_RESIDUAL_NORM = np.mean(list(MEAN_NORMS.values()))\n",
        "STEERING_ALPHAS = [ratio * MEAN_RESIDUAL_NORM for ratio in CONFIG.steering_strength_ratios]\n",
        "\n",
        "print(f\"\\n\u2713 Multi-layer calibration complete!\")\n",
        "print(f\"  Mean norms per layer: {MEAN_NORMS}\")\n",
        "print(f\"  Overall MEAN_RESIDUAL_NORM = {MEAN_RESIDUAL_NORM:.2f}\")\n",
        "print(f\"  STEERING_ALPHAS = {[f'{a:.2f}' for a in STEERING_ALPHAS]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b42b71d6",
      "metadata": {
        "id": "b42b71d6"
      },
      "source": [
        "## \ud83d\udcca Load Datasets from HuggingFace\n",
        "\n",
        "We use three datasets:\n",
        "- **NQ-Open**: Natural Questions for hallucination detection (has ground truth answers)\n",
        "- **RealToxicityPrompts**: Challenging prompts that often elicit toxic responses\n",
        "- **Anthropic HH**: Harmless/helpful prompts as a safe baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26d59ac3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694,
          "referenced_widgets": [
            "8b29442a04be476eb769379db3890c02",
            "c8fd7cf0029d4fb3965a41f3eb7cb47a",
            "f0a97cf4b04642c58cccd2c620bdaf8a",
            "cad30d7d6f324950a46794f3c5010794",
            "faed117f2fde43a0845800afda3d86ea",
            "d3e7c4b4d80a45a3b2a7809c1f2068c2",
            "d4aaae0cd60c4a18aa482f3ebf8f640a",
            "3550707c1df04be2a3d7073012a5dffc",
            "4972b0dbcacc441396a8044170094a30",
            "2ae534eb17924ad78ea576d822a404c0",
            "b8987d346a884ab6ac2ced447570d7b8",
            "ad37989df4c24f318a93605d9a079579",
            "ec33dcb898a048d1ac81433577252e14",
            "ceb1ec6d046d414dbf7c687c4aeef01d",
            "1f62c720e74e43d6b02b49e19c11392d",
            "c32effd500104449828ffeea8c293f42",
            "d4aef49ce3fb465c9c88b6536b0b3ee4",
            "fd81b2d2c4954b1f8d3e04e049674c23",
            "9d2b773ac83c4ac5a6af41c1f6edc606",
            "5962051a02264f55a12ee95ac764ef7f",
            "d3a2398d9786481aa549d862087310af",
            "782a10655482406ba04d0402da65737d",
            "91cadff15c624c8db525a7ee56734bc4",
            "8a7e23cd754a4b6fa19d89f462d2500f",
            "0142ca620e1946ae8ef205be9ba6b0d5",
            "b7fe1b1fbcfd4c14ad74d02b3303d34e",
            "333be3ab87ef4f46865cd3a8df37898d",
            "cd672f72150b4418bd559979bc4950fc",
            "1a962865abd94cd9ae790ec1ff3639da",
            "1102acd6d4eb4eec9ff38c1c05dc7037",
            "3929a8b429274b83aea243d6875b28dd",
            "dcd3e473275e477a852903ae0a8b47a1",
            "420673e6de0c4924ac72b41db43d123f",
            "afbbf1c1752b48808a1f31ce8d19a8e0",
            "21fa3f83409c481fb4fd54c7f2f05ccf",
            "47d7efd943af4e90ade7aa8ac9fcccfa",
            "42d6e12253ec4948a347717845736686",
            "b901e998ec65401aa96c08eed9fd79c8",
            "04b57e9d91974be3b4aeecad434497f6",
            "e6a646467fec47aa8574478311d8f133",
            "53089bea2028481aaa719038f8f6a109",
            "41b95f7b66474957ae1f1748ab61b66b",
            "bb7fcdd09bd74a68af6bf6465a14247e",
            "612fcb6038474368afc621890c39e8be",
            "da6b27c9096a49e4a2374d01305c5385",
            "78318f8c58cf47869a7a007e368ae1cf",
            "d4f363af78454146a632c3d4b18d387a",
            "682719c388744bb5ad6e05053522e726",
            "2667653c9d5f432da825d08ccdb2d7b9",
            "58442bc6284542b286e2cc6d611e33fe",
            "35f359ee956c4420ad89b6a82eed70af",
            "af03ab1f7a2b4b29b4812c8109756a22",
            "f097a5ea8c0a42189ba2e04ef2f3ac30",
            "f58d378b008f41b6a31074d0cd5c0269",
            "a5a07ee64e5c4791ae22d01a572448e4",
            "b385888086a145de95cf39116f936b06",
            "c4193ba715f84542a934637bd6c177f2",
            "b3c13ed80cb74bb6b29df06a8989ede6",
            "3280cc2df37b4ab1923c90225c9804a0",
            "c83e774c4c1e4e33b9f2a696cc5e42e0",
            "f4351d54b65d4afcb225ffeecd607fb0",
            "f17bcfc48f0c40e0b96c1f6b778be247",
            "75490d7528dd4b50bf1e5fb9f8211ff4",
            "fa2337c7b4f3464490691cc870ccafbe",
            "f9d469e595604a34aa0e2b0b6da3cae4",
            "0f1be648fb414f608f48110ca94cc881",
            "89dab6b3c50f4fe3bb54c9ae6795a57f",
            "44102fb8af28450888e88f10a5799dc9",
            "ef0be40028244435ac9d4483de65509f",
            "ff80f0932944437990c6578742d6f313",
            "0f6c9937cf4c4ee2984240d2e63a8f99",
            "ac4ae17f07114305a14fef7189dd2adf",
            "d4dee1a2c5694d178af5807d4c6a4846",
            "391b402e118f440f9db99d1a3fd7dd0d",
            "392baaec91e94237a1f3587087a1869f",
            "1eae668bacd644bc88a958c20b79bf75",
            "480c249c73d6485b8d7d44cdc193e31e",
            "200a442843c948dabea5fe8ac8ccd7c5",
            "5e145492d0ff4002a500b2c9715cffcd",
            "cd963e91a67a4029a2c7d3a1f850deb0",
            "e3ed10cadd614488b97b8f329c316df4",
            "cbf084c26be84bb38a5c38dfdcd87307",
            "5637696fe03f4851bea11186ec17eb6d",
            "44afe109f7ec41ad8694f638a6f192ab",
            "86a1eabcc3fe4eb0af9a8b9781c75575",
            "59a80d12e11741e4bcfc16e7610b472d",
            "d454b670bfce42bc962ccf70359ae314",
            "f984813465d048c39dcabaca10c0d8ef",
            "2abed43c05ea4593b6d8b09f9889540a",
            "2c420dab2b4c43d6ac39030004b15306",
            "43702547a2c0496f823bc49e64c3758c",
            "f13679ba4ed0461fa8154bb918878f87",
            "2e32bbf51d514fd1ad4bc83ac606b5e8",
            "9eb116ec3791443daf960a9deb718c0a",
            "762a025a482a41c0a0f31d4ffddb16b6",
            "aa8e0cb9a7cc4f0585bfa1739f23226b",
            "941520579f5e4bd69ba1f54002adf9b9",
            "17d140b802694b89a2a718b23603203c",
            "366a8b6c9d3b4fa9a2e1314e23dd3ff3",
            "4a79ca51405442b6ba7b5fa42d8d7712",
            "9e9f659a7820474790934a5b9153ec30",
            "fa1fad19a381425593eab7fa9a972a58",
            "9f1d6eed25b34df1a1bc6d6810c645d5",
            "3f64ea8a121c457d938438b291c77207",
            "6cf5f2f55a304a2c969320d5427c6f82",
            "7c668a4936aa41258c1d7042f9bd4fa4",
            "1902617935834394a60da2b8cecb639e",
            "e556aeb5bb6142188320cdbfb7f21c26",
            "0af308d55c7c466caff9371ac83da72a",
            "4783834e679349909a24bfa9e697479a",
            "aa8f1b012cbe4282aa6696f03241ad69",
            "9507bcb76894454cbfc6423a0a13a728",
            "13f1e52ed45442a58fc0761aa1028a66",
            "a5020f97e2154a91a0fb12d2fc52c875",
            "8e004a2eea4c4231a31eb80d6576949d",
            "19eb27d046bc4f9ba189927e81310fd5",
            "0898a047cb674e29acdafa462ea02445",
            "90f71e4b6f164e37803bfee20394d5f6",
            "33f5b91c4e8d47a499ac40ae52e5c8fa",
            "48a987a702cb44ac9360e6c496684ed6",
            "b9cb38bdb5f34002ba49d7e94970ac74",
            "0548bee2ff944bcaa39fd5af13e56659",
            "c88cad84490944b0a2d519cea874e4db",
            "9311f1a90d064d01bc30791ade97e656",
            "f7577d44870141d684eeccc3ed53bdf3",
            "83a8e0ad4330455fbc13d3e135d2d49a",
            "900265703fbb4893abc37902df65f7a4",
            "2483d767f6644baea120b89630d8da16",
            "bce3043ba7854450b3ec5728b7b27c20",
            "7a9ef453d42245d6802aff878a0032e2",
            "ae9450a5d15c434ba342c298524cbd5b",
            "521fd5c0a8f44dbdac0e0bc9efc3d472",
            "3b169253f3dc48f29a5f8ab0c25bfde5",
            "73c6a4a5f5bc4dfb9c1e9ef9a7fdaa70",
            "9c4cd7da996b49feaacb5182694bf9eb",
            "0a7b7b35c4da4dc9924cd99113992a7b",
            "a9799d6d194a4ca09b857a499a150372",
            "4253e6cb0bd64e4db9e3366564d45a4b",
            "0ed1eeeba18843fdb4d8281fe0805b78",
            "251431ffcc7545078e82ca3f4cfbf840",
            "4db2a4d1e26244949fee04814650f802",
            "37581449631b4bcebb92c0bbfd6c22cc",
            "2557c962d6034818a1904697b4d728d1",
            "e457a40ffa7f4391925eb564e791ab95",
            "38ae4cf3f4fe449e9f82bb912789e833",
            "69703438ccc54ea8aba4933c24e9fd5c",
            "f555805d1f234dbfb6d2ba00fd8b5c5d",
            "68bc6410e6fe498abc4fce203ddf488a",
            "fda4323d845a4bacac8a4174c3f30e3a",
            "d48f1fdd563c48988b26d84334852710",
            "a004e74d10c44702982cedee2f952a8b",
            "ed05a7d794c44854aa3653f1db319603",
            "7e82bfcd5c554f86ab573196415c1648",
            "4549be00734a477fa74ed9d764174e26",
            "988bd3d19c16465fb9a2ca6254ff353a",
            "2cd36c38d6ae4942bab160e515846a5c",
            "bcca42bb71f44bb580e3d0f0ec9ac0b0",
            "a4a0b54c484e43f0adcb47eb95cc45c1",
            "f4cbd84967444a2abc6de4283e91925d",
            "bb30c70762bf45128f5bce9b6ccc9731",
            "3fc67e2971c94462ae178682a21d6a53",
            "f0e0434375e04504b9e45be0bd9844e4",
            "fdd91ae689224ea281747b11e699b8d2",
            "351ba14b69bc481fbc20cd0f50acd1d1",
            "f29140a8d67e499a8edc3ef7640e471a",
            "cf68e6c0116d43a3b3b121c20d9559d2",
            "576c183e5b4a47d48d0b86a1d58bed4c",
            "c2bee08202464aff87dcdfe12dd735ad",
            "9a42a079a4c94448b6e7088b5c8d4233",
            "55845afde7d947d39fa641ca2c2b6883",
            "804f898bf618429e937d49825341e235",
            "ccafa8921f0d44d299d471f0a993ea5a",
            "6dc139b845ab460d8bfa70df218ba952",
            "1cff2008a47e4608b9d1c559142011fc",
            "8c2ad575a0374d06bc2e3cb9d68cef81",
            "b3c6d36576764c10b14db1a48250cca3",
            "54dea2c8c0314f63babc0656df8dfe84",
            "cddec42ef8a84a11a08cdd33e85c6596",
            "d50b10a0a52f4c91a39263e6526c8a45",
            "28790bd94b384dbe9ecd28d0e8733277",
            "102b5448990449dfa00c6ea6d00c68ac",
            "ff04fc6fc21a4d3388226072c15913c6",
            "20d03c004914421fbb95875c5002d992",
            "7aa67e4a85b24ada979968b081cc5f20",
            "dfa353ea4f5545bebf57dea14c90944e",
            "a999cb161b13475587e20afb070d0836",
            "156318ecd0c64c6397e8f4dac580f9b7",
            "e5affe6888444e9f9f66d249b92a9ae0",
            "512a71c1d59c4263b9aaada1e4f3d505",
            "082568c5ed4643dd9f62a9e2492ebf9c",
            "b71e57e1b4a04556979dd2bffb280190",
            "e83a36d1bc7c413b9cd73e184b467dd2",
            "0c78c9ee27db4f4f838077dfb67cae5d",
            "b73105f6b2814fe4867e3f15ad6c828a",
            "ad1fbb8fbc804c1f82f7d059a4f9bc4e",
            "92c4943f1e564c64bc2e5d303435b7a3",
            "f674cebb20914ae496783dbc9ec72748",
            "2a731bc5cd3d42d389de4f04186a07b6",
            "17f0c0cdb44e4dfb83d7e8252a3e59ae",
            "3144d5306ef247c7bdc955bdd06ec6f3",
            "eba77bb24b254dc5bd8039547899b77e",
            "5d1a3fee099146cdad1456f8da689549",
            "d2a76e7214244919b46574b2e18541d1",
            "7b56754b6c4d47adb63eae5cc6e2d063",
            "0e7b12af98cc49c891a3665c39ce9604",
            "5004e1256d7441e88f06488fd627ae80",
            "7c1100f941924476980e8b90544ea1e1",
            "3898721e8ef048d796066c31025d3028",
            "460887b0ba7f4ab485683e47aef96662"
          ]
        },
        "id": "26d59ac3",
        "outputId": "341179d2-6c2a-4dda-c815-bb5f5733b9a5"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load datasets\n",
        "print(\"Loading datasets from HuggingFace...\")\n",
        "\n",
        "# NQ-Open: Question answering with ground truth\n",
        "nq_dataset = load_dataset(\"nq_open\", split=\"validation\")\n",
        "print(f\"\u2713 NQ-Open: {len(nq_dataset)} samples\")\n",
        "\n",
        "# RealToxicityPrompts: Toxic prompt continuation\n",
        "rtp_dataset = load_dataset(\"allenai/real-toxicity-prompts\", split=\"train\")\n",
        "print(f\"\u2713 RealToxicityPrompts: {len(rtp_dataset)} samples\")\n",
        "\n",
        "# Anthropic HH: Safe conversations\n",
        "hh_dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"test\")\n",
        "print(f\"\u2713 Anthropic HH: {len(hh_dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23a83ca5",
      "metadata": {
        "id": "23a83ca5"
      },
      "source": [
        "## \ud83d\udd2c Capture Activations & Generate Labels\n",
        "\n",
        "For each prompt, we:\n",
        "1. Run it through Gemma to capture layer 12 residuals\n",
        "2. Encode the residual with SAE to get sparse feature codes\n",
        "3. Generate a label (hallucination or toxicity)\n",
        "\n",
        "### Improved Hallucination Labeling\n",
        "\n",
        "The original code used answer length as a proxy for hallucination, which produced very few positive samples.\n",
        "\n",
        "**Better approach**: Compare model answers to the **ground truth answers** in NQ-Open using string matching.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc140c16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc140c16",
        "outputId": "c7d66351-eff0-401f-b4b7-d454a73d3ee5"
      },
      "outputs": [],
      "source": [
        "def check_hallucination(model_answer: str, ground_truth_answers: list) -> bool:\n",
        "    \"\"\"\n",
        "    Check if model answer is a hallucination by comparing to ground truth.\n",
        "    Returns True if the answer is likely hallucinated.\n",
        "    \"\"\"\n",
        "    model_answer_lower = model_answer.lower().strip()\n",
        "\n",
        "    if len(model_answer_lower.split()) < 2:\n",
        "        return True\n",
        "\n",
        "    for gt in ground_truth_answers:\n",
        "        gt_lower = gt.lower().strip()\n",
        "        if gt_lower in model_answer_lower or model_answer_lower in gt_lower:\n",
        "            return False\n",
        "\n",
        "        gt_words = set(gt_lower.split())\n",
        "        answer_words = set(model_answer_lower.split())\n",
        "        overlap = len(gt_words & answer_words)\n",
        "        if overlap >= min(2, len(gt_words)):\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# BATCH SIZES\n",
        "# ============================================================================\n",
        "DATA_BATCH_SIZE = CONFIG.data_batch_size\n",
        "STEERING_BATCH_SIZE = CONFIG.steering_batch_size\n",
        "\n",
        "\n",
        "def process_nq_open_multilayer(dataset, limit: int, batch_size: int = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Process NQ-Open dataset for hallucination detection - MULTI-LAYER VERSION.\n",
        "    Stores codes for each layer individually AND concatenated.\n",
        "    \"\"\"\n",
        "    if batch_size is None:\n",
        "        batch_size = DATA_BATCH_SIZE\n",
        "\n",
        "    questions = []\n",
        "    ground_truths = []\n",
        "\n",
        "    for i, row in enumerate(dataset):\n",
        "        if i >= limit:\n",
        "            break\n",
        "        questions.append(row['question'])\n",
        "        ground_truths.append(row['answer'])\n",
        "\n",
        "    print(f\"  Processing {len(questions)} questions in batches of {batch_size}...\")\n",
        "\n",
        "    records = []\n",
        "\n",
        "    for batch_start in tqdm(range(0, len(questions), batch_size), desc=\"Processing NQ-Open\"):\n",
        "        batch_end = min(batch_start + batch_size, len(questions))\n",
        "        batch_questions = questions[batch_start:batch_end]\n",
        "        batch_gt = ground_truths[batch_start:batch_end]\n",
        "\n",
        "        # Batch generate with multi-layer residual capture\n",
        "        results = gemma.generate_batch(batch_questions, max_new_tokens=50)\n",
        "\n",
        "        for j, (question, gt, result) in enumerate(zip(batch_questions, batch_gt, results)):\n",
        "            if not result['residuals']:\n",
        "                continue\n",
        "\n",
        "            model_answer = result['text'].replace(question, '').strip()\n",
        "\n",
        "            # Encode with multi-layer SAE - get codes for each layer AND concatenated\n",
        "            codes_dict = sae.get_codes_dict(result['residuals'])\n",
        "\n",
        "            is_hallucinated = check_hallucination(model_answer, gt)\n",
        "\n",
        "            record = {\n",
        "                'id': batch_start + j,\n",
        "                'prompt': question,\n",
        "                'model_answer': model_answer,\n",
        "                'ground_truth': gt,\n",
        "                'label': int(is_hallucinated),\n",
        "            }\n",
        "            # Add codes for each layer and multi-layer\n",
        "            for key, codes in codes_dict.items():\n",
        "                record[f'codes_{key}'] = codes\n",
        "\n",
        "            records.append(record)\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    n_halluc = df['label'].sum()\n",
        "    print(f\"\u2713 NQ-Open: {len(df)} samples, {n_halluc} hallucinated ({n_halluc/len(df)*100:.1f}%)\")\n",
        "    print(f\"  Codes stored: {[k for k in df.columns if k.startswith('codes_')]}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# Alias for compatibility\n",
        "def process_nq_open(dataset, limit: int) -> pd.DataFrame:\n",
        "    return process_nq_open_multilayer(dataset, limit, batch_size=DATA_BATCH_SIZE)\n",
        "\n",
        "\n",
        "print(f\"\u2713 Multi-layer hallucination detection functions defined (batch_size={DATA_BATCH_SIZE})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cb1c1c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cb1c1c7",
        "outputId": "a7ee0a05-1fd1-49cb-991c-35612d1a6376"
      },
      "outputs": [],
      "source": [
        "def process_toxicity_multilayer(dataset, name: str, limit: int,\n",
        "                                  text_field: str = 'prompt',\n",
        "                                  is_toxic_baseline: bool = True,\n",
        "                                  batch_size: int = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Process toxicity dataset - MULTI-LAYER VERSION.\n",
        "    Stores codes for each layer individually AND concatenated.\n",
        "    \"\"\"\n",
        "    if batch_size is None:\n",
        "        batch_size = DATA_BATCH_SIZE\n",
        "\n",
        "    prompts = []\n",
        "    indices = []\n",
        "\n",
        "    for i, row in enumerate(dataset):\n",
        "        if len(prompts) >= limit:\n",
        "            break\n",
        "\n",
        "        if text_field == 'prompt' and isinstance(row.get('prompt'), dict):\n",
        "            prompt = row['prompt'].get('text', '')\n",
        "        elif text_field == 'chosen':\n",
        "            text = row['chosen']\n",
        "            if 'Human:' in text:\n",
        "                prompt = text.split('Human:')[1].split('Assistant:')[0].strip()\n",
        "            else:\n",
        "                prompt = text[:200]\n",
        "        else:\n",
        "            prompt = row.get(text_field, '')\n",
        "\n",
        "        if not prompt or len(prompt) < 5:\n",
        "            continue\n",
        "\n",
        "        prompts.append(prompt[:500])\n",
        "        indices.append(i)\n",
        "\n",
        "    print(f\"  Processing {len(prompts)} prompts in batches of {batch_size}...\")\n",
        "\n",
        "    records = []\n",
        "\n",
        "    for batch_start in tqdm(range(0, len(prompts), batch_size), desc=f\"Processing {name}\"):\n",
        "        batch_end = min(batch_start + batch_size, len(prompts))\n",
        "        batch_prompts = prompts[batch_start:batch_end]\n",
        "        batch_indices = indices[batch_start:batch_end]\n",
        "\n",
        "        results = gemma.generate_batch(batch_prompts, max_new_tokens=50)\n",
        "\n",
        "        for j, (prompt, idx, result) in enumerate(zip(batch_prompts, batch_indices, results)):\n",
        "            if not result['residuals']:\n",
        "                continue\n",
        "\n",
        "            completion = result['text'].replace(prompt, '').strip()\n",
        "\n",
        "            # Encode with multi-layer SAE\n",
        "            codes_dict = sae.get_codes_dict(result['residuals'])\n",
        "\n",
        "            tox_score = tox.score(completion)\n",
        "\n",
        "            record = {\n",
        "                'id': idx,\n",
        "                'prompt': prompt,\n",
        "                'completion': completion,\n",
        "                'toxicity_probability': tox_score.probability,\n",
        "                'label': tox_score.label,\n",
        "            }\n",
        "            for key, codes in codes_dict.items():\n",
        "                record[f'codes_{key}'] = codes\n",
        "\n",
        "            records.append(record)\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    n_toxic = df['label'].sum()\n",
        "    print(f\"\u2713 {name}: {len(df)} samples, {n_toxic} toxic ({n_toxic/len(df)*100:.1f}%)\")\n",
        "    print(f\"  Codes stored: {[k for k in df.columns if k.startswith('codes_')]}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# Alias for compatibility\n",
        "def process_toxicity_dataset(dataset, name: str, limit: int,\n",
        "                              text_field: str = 'prompt',\n",
        "                              is_toxic_baseline: bool = True) -> pd.DataFrame:\n",
        "    return process_toxicity_multilayer(dataset, name, limit, text_field, is_toxic_baseline, DATA_BATCH_SIZE)\n",
        "\n",
        "\n",
        "print(f\"\u2713 Multi-layer toxicity processing functions defined (batch_size={DATA_BATCH_SIZE})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6d98a2e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6d98a2e",
        "outputId": "0f97bee1-955a-478c-c634-ab0c25d76228"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PROCESS ALL DATASETS (BATCHED - ~5-10x FASTER!)\n",
        "# ============================================================================\n",
        "# Using batch_size=64 for data processing on A100 80GB\n",
        "# Processing ALL available data:\n",
        "#   - NQ-Open: all ~3610 validation samples\n",
        "#   - Anthropic HH: all ~8552 samples\n",
        "#   - RealToxicityPrompts: sampled to match HH count (balanced dataset)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"PROCESSING DATASETS (FULL DATA)\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Using batch_size={DATA_BATCH_SIZE} for GPU acceleration\")\n",
        "print(f\"  NQ-Open limit: {CONFIG.nq_sample_limit} (use all available)\")\n",
        "print(f\"  HH limit: {CONFIG.hh_sample_limit} (use all available)\")\n",
        "print(f\"  RTP limit: {CONFIG.rtp_sample_limit} (balanced with HH)\\n\")\n",
        "\n",
        "# 1. Process NQ-Open (Hallucination) - use all available\n",
        "print(\"1/3: Processing NQ-Open for hallucination detection...\")\n",
        "nq_df = process_nq_open(nq_dataset, limit=CONFIG.nq_sample_limit)\n",
        "\n",
        "# 2. Process Anthropic HH (Safe) - use all available\n",
        "print(\"\\n2/3: Processing Anthropic HH (safe baseline)...\")\n",
        "hh_df = process_toxicity_dataset(hh_dataset, \"HH\", limit=CONFIG.hh_sample_limit,\n",
        "                                  text_field='chosen', is_toxic_baseline=False)\n",
        "\n",
        "# 3. Process RealToxicityPrompts (Toxic) - sample to balance with HH\n",
        "# Randomly shuffle RTP to get different samples each run\n",
        "import random\n",
        "random.seed(CONFIG.seed)\n",
        "rtp_indices = list(range(len(rtp_dataset)))\n",
        "random.shuffle(rtp_indices)\n",
        "# Create a shuffled view of the dataset\n",
        "rtp_shuffled = rtp_dataset.select(rtp_indices[:CONFIG.rtp_sample_limit])\n",
        "\n",
        "print(f\"\\n3/3: Processing RealToxicityPrompts (sampled {CONFIG.rtp_sample_limit} to balance with HH)...\")\n",
        "rtp_df = process_toxicity_dataset(rtp_shuffled, \"RTP\", limit=CONFIG.rtp_sample_limit,\n",
        "                                   text_field='prompt', is_toxic_baseline=True)\n",
        "\n",
        "# Combine RTP + HH for safety task\n",
        "safety_df = pd.concat([rtp_df, hh_df], ignore_index=True)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"DATA PREPARATION COMPLETE!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\nHallucination task (NQ-Open):\")\n",
        "print(f\"  Total: {len(nq_df)}, Hallucinated: {nq_df['label'].sum()} ({nq_df['label'].mean()*100:.1f}%), \"\n",
        "      f\"Correct: {len(nq_df) - nq_df['label'].sum()}\")\n",
        "print(f\"\\nSafety task (RTP + HH balanced):\")\n",
        "print(f\"  Total: {len(safety_df)}\")\n",
        "print(f\"  From RTP: {len(rtp_df)}, toxic outputs: {rtp_df['label'].sum()}\")\n",
        "print(f\"  From HH:  {len(hh_df)}, toxic outputs: {hh_df['label'].sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45e08306",
      "metadata": {
        "id": "45e08306"
      },
      "source": [
        "---\n",
        "\n",
        "# \ud83d\udd0d Part 2: Feature Discovery & Detection\n",
        "\n",
        "Now we identify which SAE features correlate with risky behavior:\n",
        "\n",
        "- **F\u207a (Harmful features)**: Activate when the model hallucinates or generates toxic content\n",
        "- **F\u207b (Protective features)**: Activate when the model is factual or generates safe content\n",
        "\n",
        "We then train logistic regression classifiers and report:\n",
        "- **Accuracy**\n",
        "- **F1 Score**\n",
        "- **ROC-AUC**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67aa5fad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67aa5fad",
        "outputId": "cdbe212f-3f74-42e0-8b3c-c32dd0a2b49a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, f1_score\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "\n",
        "def discover_features_multilayer(df: pd.DataFrame, task_name: str, top_k: int = 50) -> Dict:\n",
        "    \"\"\"\n",
        "    Find SAE features correlated with the label for EACH layer AND multi-layer.\n",
        "\n",
        "    Returns:\n",
        "        Dict with keys 'layer_12', 'layer_20', 'multi_layer', each containing:\n",
        "            - all_features: DataFrame with all feature correlations\n",
        "            - f_plus: Top features with positive correlation (harmful)\n",
        "            - f_minus: Top features with negative correlation (protective)\n",
        "    \"\"\"\n",
        "    labels = df['label'].values\n",
        "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{task_name.upper()} MULTI-LAYER FEATURE DISCOVERY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"  Label distribution: {dict(zip(unique_labels.astype(int), counts))}\")\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Process each layer configuration\n",
        "    layer_configs = ['layer_12', 'layer_20', 'multi_layer']\n",
        "\n",
        "    for layer_key in layer_configs:\n",
        "        codes_col = f'codes_{layer_key}'\n",
        "        if codes_col not in df.columns:\n",
        "            print(f\"  \u26a0\ufe0f Skipping {layer_key}: column {codes_col} not found\")\n",
        "            continue\n",
        "\n",
        "        codes_matrix = np.stack(df[codes_col].values)\n",
        "        n_features = codes_matrix.shape[1]\n",
        "\n",
        "        print(f\"\\n  --- {layer_key.upper()} ({n_features} features) ---\")\n",
        "\n",
        "        if len(unique_labels) < 2:\n",
        "            print(f\"    \u26a0\ufe0f Only one class - using variance-based selection\")\n",
        "            feature_variance = codes_matrix.var(axis=0)\n",
        "            feature_mean = codes_matrix.mean(axis=0)\n",
        "\n",
        "            correlations = []\n",
        "            for feat_idx in range(n_features):\n",
        "                if feature_variance[feat_idx] > 0:\n",
        "                    correlations.append({\n",
        "                        'feature_id': feat_idx,\n",
        "                        'correlation': feature_variance[feat_idx],\n",
        "                        'raw_corr': feature_mean[feat_idx]\n",
        "                    })\n",
        "\n",
        "            all_features = pd.DataFrame(correlations).sort_values('correlation', ascending=False)\n",
        "            top_features = all_features.head(top_k)\n",
        "            median_val = top_features['raw_corr'].median()\n",
        "            f_plus = top_features[top_features['raw_corr'] >= median_val].head(top_k // 2)\n",
        "            f_minus = top_features[top_features['raw_corr'] < median_val].head(top_k // 2)\n",
        "        else:\n",
        "            correlations = []\n",
        "            for feat_idx in range(n_features):\n",
        "                feat_values = codes_matrix[:, feat_idx]\n",
        "                if feat_values.std() > 0:\n",
        "                    corr = np.corrcoef(feat_values, labels)[0, 1]\n",
        "                    if not np.isnan(corr):\n",
        "                        correlations.append({\n",
        "                            'feature_id': feat_idx,\n",
        "                            'correlation': abs(corr),\n",
        "                            'raw_corr': corr\n",
        "                        })\n",
        "\n",
        "            all_features = pd.DataFrame(correlations).sort_values('correlation', ascending=False)\n",
        "            f_plus = all_features[all_features['raw_corr'] > 0].head(top_k // 2)\n",
        "            f_minus = all_features[all_features['raw_corr'] < 0].head(top_k // 2)\n",
        "\n",
        "        print(f\"    Total features with variance: {len(all_features)}\")\n",
        "        f_plus_mean = f_plus['raw_corr'].mean() if len(f_plus) > 0 else 0\n",
        "        f_minus_mean = f_minus['raw_corr'].mean() if len(f_minus) > 0 else 0\n",
        "        print(f\"    F\u207a (harmful): {len(f_plus)} features, mean corr: {f_plus_mean:.4f}\")\n",
        "        print(f\"    F\u207b (protective): {len(f_minus)} features, mean corr: {f_minus_mean:.4f}\")\n",
        "\n",
        "        results[layer_key] = {\n",
        "            'all_features': all_features,\n",
        "            'f_plus': f_plus,\n",
        "            'f_minus': f_minus\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"\u2713 Multi-layer feature discovery function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f982bfea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f982bfea",
        "outputId": "b3add923-fb52-408a-cdd6-126e7a1ea912"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MULTI-LAYER FEATURE DISCOVERY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MULTI-LAYER FEATURE DISCOVERY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "TOP_K = CONFIG.top_k_features\n",
        "print(f\"Using top_k = {TOP_K} features per layer configuration\")\n",
        "\n",
        "# Discover features for each layer AND multi-layer\n",
        "halluc_features = discover_features_multilayer(nq_df, \"Hallucination\", top_k=TOP_K)\n",
        "safety_features = discover_features_multilayer(safety_df, \"Safety\", top_k=TOP_K)\n",
        "\n",
        "# Store feature IDs for steering (25 features of each type per layer)\n",
        "N_STEERING_FEATURES = 25\n",
        "\n",
        "# Extract feature IDs for each configuration\n",
        "halluc_feature_ids = {}\n",
        "safety_feature_ids = {}\n",
        "\n",
        "for layer_key in ['layer_12', 'layer_20', 'multi_layer']:\n",
        "    if layer_key in halluc_features:\n",
        "        halluc_feature_ids[layer_key] = {\n",
        "            'f_plus': halluc_features[layer_key]['f_plus']['feature_id'].values[:N_STEERING_FEATURES],\n",
        "            'f_minus': halluc_features[layer_key]['f_minus']['feature_id'].values[:N_STEERING_FEATURES]\n",
        "        }\n",
        "    if layer_key in safety_features:\n",
        "        safety_feature_ids[layer_key] = {\n",
        "            'f_plus': safety_features[layer_key]['f_plus']['feature_id'].values[:N_STEERING_FEATURES],\n",
        "            'f_minus': safety_features[layer_key]['f_minus']['feature_id'].values[:N_STEERING_FEATURES]\n",
        "        }\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"STEERING FEATURE IDS SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "for layer_key in ['layer_12', 'layer_20', 'multi_layer']:\n",
        "    if layer_key in halluc_feature_ids:\n",
        "        print(f\"\\n{layer_key}:\")\n",
        "        print(f\"  Halluc F\u207a: {halluc_feature_ids[layer_key]['f_plus'][:5]}... ({len(halluc_feature_ids[layer_key]['f_plus'])} total)\")\n",
        "        print(f\"  Halluc F\u207b: {halluc_feature_ids[layer_key]['f_minus'][:5]}... ({len(halluc_feature_ids[layer_key]['f_minus'])} total)\")\n",
        "        print(f\"  Safety F\u207a: {safety_feature_ids[layer_key]['f_plus'][:5]}... ({len(safety_feature_ids[layer_key]['f_plus'])} total)\")\n",
        "        print(f\"  Safety F\u207b: {safety_feature_ids[layer_key]['f_minus'][:5]}... ({len(safety_feature_ids[layer_key]['f_minus'])} total)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aedecd78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "id": "aedecd78",
        "outputId": "146b5af6-d4b6-49d9-bea3-022e267783ee"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZE TOP FEATURES PER LAYER\n",
        "# ============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "layer_configs = ['layer_12', 'layer_20', 'multi_layer']\n",
        "titles = ['Layer 12 Features', 'Layer 20 Features', 'Multi-Layer Features']\n",
        "\n",
        "# Hallucination features (top row)\n",
        "for col, (layer_key, title) in enumerate(zip(layer_configs, titles)):\n",
        "    ax = axes[0, col]\n",
        "    if layer_key in halluc_features:\n",
        "        top_feat = halluc_features[layer_key]['all_features'].head(15)\n",
        "        colors = ['#e74c3c' if c > 0 else '#27ae60' for c in top_feat['raw_corr']]\n",
        "        ax.barh(range(len(top_feat)), top_feat['raw_corr'], color=colors)\n",
        "        ax.set_yticks(range(len(top_feat)))\n",
        "        ax.set_yticklabels([f\"F{int(f)}\" for f in top_feat['feature_id']])\n",
        "        ax.set_xlabel('Correlation with Hallucination')\n",
        "        ax.set_title(f'{title}\\n(Red=F\u207a, Green=F\u207b)')\n",
        "        ax.axvline(x=0, color='black', linewidth=0.5)\n",
        "        ax.invert_yaxis()\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n",
        "        ax.set_title(title)\n",
        "\n",
        "# Safety features (bottom row)\n",
        "for col, (layer_key, title) in enumerate(zip(layer_configs, titles)):\n",
        "    ax = axes[1, col]\n",
        "    if layer_key in safety_features:\n",
        "        top_feat = safety_features[layer_key]['all_features'].head(15)\n",
        "        colors = ['#e74c3c' if c > 0 else '#27ae60' for c in top_feat['raw_corr']]\n",
        "        ax.barh(range(len(top_feat)), top_feat['raw_corr'], color=colors)\n",
        "        ax.set_yticks(range(len(top_feat)))\n",
        "        ax.set_yticklabels([f\"F{int(f)}\" for f in top_feat['feature_id']])\n",
        "        ax.set_xlabel('Correlation with Toxicity')\n",
        "        ax.set_title(f'{title}\\n(Red=F\u207a, Green=F\u207b)')\n",
        "        ax.axvline(x=0, color='black', linewidth=0.5)\n",
        "        ax.invert_yaxis()\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n",
        "        ax.set_title(title)\n",
        "\n",
        "# Add row labels\n",
        "axes[0, 0].set_ylabel('HALLUCINATION\\n\\nFeature', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('SAFETY\\n\\nFeature', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Multi-Layer Feature Comparison: Top 15 Features per Configuration',\n",
        "             fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"\u2713 Multi-layer feature visualization complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d42a736e",
      "metadata": {
        "id": "d42a736e"
      },
      "source": [
        "## \ud83c\udfaf Train Detectors\n",
        "\n",
        "Train logistic regression classifiers using the discovered features to detect:\n",
        "1. **Hallucination detector**: Predicts if the model's answer is hallucinated\n",
        "2. **Safety detector**: Predicts if the model's output is toxic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d39e6d3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d39e6d3a",
        "outputId": "eed0eb6c-229b-41bc-e921-6615d4c7e077"
      },
      "outputs": [],
      "source": [
        "def train_detector_multilayer(df: pd.DataFrame, features_dict: Dict, task_name: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Train logistic regression detectors for EACH layer configuration and compare.\n",
        "\n",
        "    Returns:\n",
        "        Dict with keys 'layer_12', 'layer_20', 'multi_layer', each containing:\n",
        "            - clf: Trained classifier\n",
        "            - metrics: Dict with accuracy, f1, auroc\n",
        "            - feature_indices: Array of feature indices used\n",
        "    \"\"\"\n",
        "    y = df['label'].values\n",
        "    unique, counts = np.unique(y, return_counts=True)\n",
        "    class_dist = dict(zip(unique.astype(int), counts))\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{task_name.upper()} MULTI-LAYER DETECTOR COMPARISON\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"  Class distribution: {class_dist}\")\n",
        "\n",
        "    if len(unique) < 2:\n",
        "        print(f\"  \u26a0\ufe0f Only one class - cannot train classifiers\")\n",
        "        return {}\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for layer_key in ['layer_12', 'layer_20', 'multi_layer']:\n",
        "        codes_col = f'codes_{layer_key}'\n",
        "        if codes_col not in df.columns or layer_key not in features_dict:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n  --- {layer_key.upper()} ---\")\n",
        "\n",
        "        all_features = features_dict[layer_key]['all_features']\n",
        "        feature_indices = all_features['feature_id'].values[:50]\n",
        "\n",
        "        codes_matrix = np.stack(df[codes_col].values)\n",
        "        X = codes_matrix[:, feature_indices]\n",
        "\n",
        "        min_class = min(counts)\n",
        "        if min_class >= 10:\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=0.2, random_state=CONFIG.seed, stratify=y\n",
        "            )\n",
        "        else:\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=0.2, random_state=CONFIG.seed\n",
        "            )\n",
        "\n",
        "        clf = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=CONFIG.seed)\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = clf.predict(X_test)\n",
        "        y_prob = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "        if len(np.unique(y_test)) > 1:\n",
        "            auroc = roc_auc_score(y_test, y_prob)\n",
        "        else:\n",
        "            auroc = 0.5\n",
        "\n",
        "        metrics = {'accuracy': acc, 'f1': f1, 'auroc': auroc}\n",
        "\n",
        "        print(f\"    Accuracy: {acc:.3f}\")\n",
        "        print(f\"    F1 Score: {f1:.3f}\")\n",
        "        print(f\"    ROC-AUC:  {auroc:.3f}\")\n",
        "\n",
        "        results[layer_key] = {\n",
        "            'clf': clf,\n",
        "            'metrics': metrics,\n",
        "            'feature_indices': feature_indices\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"\u2713 Multi-layer detector training function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa05a18b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa05a18b",
        "outputId": "f3072b88-b5c4-49f3-88a3-af687bb62021"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TRAIN MULTI-LAYER DETECTORS & COMPARE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING MULTI-LAYER DETECTORS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Train hallucination detectors for each layer configuration\n",
        "halluc_detectors = train_detector_multilayer(nq_df, halluc_features, \"Hallucination\")\n",
        "\n",
        "# Train safety detectors for each layer configuration\n",
        "safety_detectors = train_detector_multilayer(safety_df, safety_features, \"Safety\")\n",
        "\n",
        "# ============================================================================\n",
        "# COMPARISON SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MULTI-LAYER DETECTION COMPARISON SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n{'Layer Config':<15} {'Task':<15} {'Accuracy':<12} {'F1 Score':<12} {'ROC-AUC':<12}\")\n",
        "print(\"-\"*66)\n",
        "\n",
        "for layer_key in ['layer_12', 'layer_20', 'multi_layer']:\n",
        "    if layer_key in halluc_detectors:\n",
        "        m = halluc_detectors[layer_key]['metrics']\n",
        "        print(f\"{layer_key:<15} {'Hallucination':<15} {m['accuracy']:<12.3f} {m['f1']:<12.3f} {m['auroc']:<12.3f}\")\n",
        "    if layer_key in safety_detectors:\n",
        "        m = safety_detectors[layer_key]['metrics']\n",
        "        print(f\"{layer_key:<15} {'Safety':<15} {m['accuracy']:<12.3f} {m['f1']:<12.3f} {m['auroc']:<12.3f}\")\n",
        "\n",
        "# Find best configuration for each task\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BEST LAYER CONFIGURATION PER TASK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for task_name, detectors in [(\"Hallucination\", halluc_detectors), (\"Safety\", safety_detectors)]:\n",
        "    if detectors:\n",
        "        best_layer = max(detectors.keys(), key=lambda k: detectors[k]['metrics']['auroc'])\n",
        "        best_auc = detectors[best_layer]['metrics']['auroc']\n",
        "        print(f\"  {task_name}: {best_layer} (ROC-AUC = {best_auc:.3f})\")\n",
        "\n",
        "        # Calculate improvement of multi-layer over single layers\n",
        "        if 'multi_layer' in detectors and 'layer_12' in detectors and 'layer_20' in detectors:\n",
        "            multi_auc = detectors['multi_layer']['metrics']['auroc']\n",
        "            l12_auc = detectors['layer_12']['metrics']['auroc']\n",
        "            l20_auc = detectors['layer_20']['metrics']['auroc']\n",
        "            best_single = max(l12_auc, l20_auc)\n",
        "            improvement = (multi_auc - best_single) / best_single * 100\n",
        "            print(f\"    Multi-layer vs best single-layer: {improvement:+.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b46e65a0",
      "metadata": {
        "id": "b46e65a0"
      },
      "source": [
        "---\n",
        "\n",
        "# \ud83c\udfae Part 3: Steering & Results\n",
        "\n",
        "Now we test whether suppressing harmful features (F\u207a) can reduce:\n",
        "1. **Hallucination rate** on NQ-Open questions\n",
        "2. **Toxicity rate** on RealToxicityPrompts\n",
        "\n",
        "**Steering Strategy**:\n",
        "- Build a steering vector from F\u207a and F\u207b features\n",
        "- Apply it at the last token during generation\n",
        "- Push F\u207a down (suppress) and optionally push F\u207b up (enhance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fe3574a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fe3574a",
        "outputId": "c93edba5-de9a-4e13-d316-e6636b92faeb"
      },
      "outputs": [],
      "source": [
        "def run_steering_at_strength_multilayer(prompts: list, ground_truths: list, task: str,\n",
        "                                         steering_vectors: Dict[int, torch.Tensor],\n",
        "                                         alpha: float, max_samples: int,\n",
        "                                         steering_mode: str = 'layer_12') -> list:\n",
        "    \"\"\"\n",
        "    Run steering experiment at a single \u03b1 value with multi-layer support.\n",
        "\n",
        "    Args:\n",
        "        steering_mode: 'layer_12' or 'layer_20'\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    test_prompts = prompts[:max_samples]\n",
        "    test_gt = ground_truths[:max_samples] if ground_truths else [None] * max_samples\n",
        "\n",
        "    # Prepare steering vectors and strengths based on mode\n",
        "    if alpha == 0:\n",
        "        vecs = None\n",
        "        strengths = None\n",
        "    else:\n",
        "        if steering_mode == 'layer_12':\n",
        "            vecs = {12: steering_vectors[12]} if 12 in steering_vectors else None\n",
        "            strengths = {12: alpha} if vecs else None\n",
        "        elif steering_mode == 'layer_20':\n",
        "            vecs = {20: steering_vectors[20]} if 20 in steering_vectors else None\n",
        "            strengths = {20: alpha} if vecs else None\n",
        "        else:  # layer_20 fallback\n",
        "            vecs = steering_vectors\n",
        "            strengths = {layer: alpha for layer in steering_vectors.keys()}\n",
        "\n",
        "    for prompt, gt in tqdm(zip(test_prompts, test_gt), total=len(test_prompts),\n",
        "                           desc=f\"\u03b1={alpha:.2f} ({steering_mode})\", leave=False):\n",
        "        result = gemma.generate(\n",
        "            prompt,\n",
        "            max_new_tokens=50,\n",
        "            steering_vectors=vecs,\n",
        "            strengths=strengths\n",
        "        )\n",
        "        text = result['text'].replace(prompt, '').strip()\n",
        "\n",
        "        if task == 'safety':\n",
        "            score = tox.score(text)\n",
        "            results.append({\n",
        "                'prompt': prompt[:100],\n",
        "                'text': text,\n",
        "                'toxicity': score.probability,\n",
        "                'is_toxic': score.label\n",
        "            })\n",
        "        else:  # hallucination\n",
        "            is_halluc = check_hallucination(text, gt) if gt else True\n",
        "            results.append({\n",
        "                'prompt': prompt[:100],\n",
        "                'text': text,\n",
        "                'ground_truth': gt,\n",
        "                'hallucinated': is_halluc,\n",
        "                'length': len(text.split())\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def run_multilayer_steering_experiment(prompts: list, ground_truths: list, task: str,\n",
        "                                        feature_ids: Dict[str, Dict[str, np.ndarray]],\n",
        "                                        alphas: list, max_samples: int) -> dict:\n",
        "    \"\"\"\n",
        "    Run steering experiments comparing:\n",
        "      1. Layer 12 only\n",
        "      2. Layer 20 only\n",
        "      3. Multi-layer (both simultaneously)\n",
        "\n",
        "    Args:\n",
        "        feature_ids: Dict with keys 'layer_12', 'layer_20', each containing 'f_plus' and 'f_minus'\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{task.upper()} MULTI-LAYER STEERING EXPERIMENT\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"  Samples: {max_samples}\")\n",
        "    print(f\"  \u03b1 values: {[f'{a:.2f}' for a in alphas]}\")\n",
        "    print(f\"  Modes: layer_12, layer_20, multi\")\n",
        "\n",
        "    # Build steering vectors for each layer using that layer's features\n",
        "    steering_vectors = {}\n",
        "\n",
        "    # Layer 12 steering vector from layer_12 features\n",
        "    if 'layer_12' in feature_ids:\n",
        "        steering_vectors[12] = sae.build_steering_vector(\n",
        "            layer_idx=12,\n",
        "            f_plus_ids=list(feature_ids['layer_12']['f_plus']),\n",
        "            f_minus_ids=list(feature_ids['layer_12']['f_minus']),\n",
        "            plus_weight=-1.0,\n",
        "            minus_weight=0.5\n",
        "        )\n",
        "        print(f\"  Layer 12 steering vector built from {len(feature_ids['layer_12']['f_plus'])} F+ and {len(feature_ids['layer_12']['f_minus'])} F- features\")\n",
        "\n",
        "    # Layer 20 steering vector from layer_20 features\n",
        "    if 'layer_20' in feature_ids:\n",
        "        steering_vectors[20] = sae.build_steering_vector(\n",
        "            layer_idx=20,\n",
        "            f_plus_ids=list(feature_ids['layer_20']['f_plus']),\n",
        "            f_minus_ids=list(feature_ids['layer_20']['f_minus']),\n",
        "            plus_weight=-1.0,\n",
        "            minus_weight=0.5\n",
        "        )\n",
        "        print(f\"  Layer 20 steering vector built from {len(feature_ids['layer_20']['f_plus'])} F+ and {len(feature_ids['layer_20']['f_minus'])} F- features\")\n",
        "\n",
        "    all_results = {\n",
        "        'task': task,\n",
        "        'alphas': alphas,\n",
        "        'n_samples': max_samples,\n",
        "        'per_mode': {}\n",
        "    }\n",
        "\n",
        "    # Test each steering mode\n",
        "    for mode in ['layer_12', 'layer_20']:\n",
        "        print(f\"\\n  === STEERING MODE: {mode.upper()} ===\")\n",
        "        all_results['per_mode'][mode] = {}\n",
        "\n",
        "        for alpha in alphas:\n",
        "            print(f\"\\n  --- \u03b1 = {alpha:.2f} ({alpha/MEAN_RESIDUAL_NORM*100:.1f}% of \u2016h\u2016) ---\")\n",
        "\n",
        "            results = run_steering_at_strength_multilayer(\n",
        "                prompts=prompts,\n",
        "                ground_truths=ground_truths,\n",
        "                task=task,\n",
        "                steering_vectors=steering_vectors,\n",
        "                alpha=alpha,\n",
        "                max_samples=max_samples,\n",
        "                steering_mode=mode\n",
        "            )\n",
        "            all_results['per_mode'][mode][alpha] = results\n",
        "\n",
        "            # Quick summary\n",
        "            if task == 'safety':\n",
        "                mean_tox = np.mean([r['toxicity'] for r in results])\n",
        "                n_toxic = sum(r['is_toxic'] for r in results)\n",
        "                print(f\"    Mean toxicity: {mean_tox:.4f}, Toxic: {n_toxic}/{len(results)}\")\n",
        "            else:\n",
        "                halluc_rate = np.mean([r['hallucinated'] for r in results])\n",
        "                print(f\"    Hallucination rate: {halluc_rate*100:.1f}%\")\n",
        "\n",
        "    return all_results\n",
        "\n",
        "\n",
        "print(\"\u2713 Multi-layer steering experiment functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53539eda",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53539eda",
        "outputId": "f24f1acb-2451-4cb9-e9e7-a0e3f24177a1"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MULTI-LAYER STEERING EXPERIMENTS\n",
        "# ============================================================================\n",
        "\n",
        "n_alphas = len(STEERING_ALPHAS)\n",
        "n_samples = CONFIG.steering_samples\n",
        "n_modes = 2  # layer_12, layer_20\n",
        "total_gens = n_samples * n_alphas * n_modes * 2  # 2 tasks\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MULTI-LAYER STEERING EXPERIMENTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Samples per experiment: {n_samples}\")\n",
        "print(f\"\u03b1 values (calibrated): {[f'{a:.2f}' for a in STEERING_ALPHAS]}\")\n",
        "print(f\"Steering modes: layer_12, layer_20, multi (both layers)\")\n",
        "print(f\"Total generations: {total_gens}\")\n",
        "\n",
        "# Get test data\n",
        "rtp_prompts = rtp_df['prompt'].tolist()\n",
        "nq_prompts = nq_df['prompt'].tolist()\n",
        "nq_ground_truth = nq_df['ground_truth'].tolist()\n",
        "\n",
        "# ============================================================================\n",
        "# SAFETY STEERING (Compare L12 vs L20 vs Multi)\n",
        "# ============================================================================\n",
        "\n",
        "safety_experiment = run_multilayer_steering_experiment(\n",
        "    prompts=rtp_prompts,\n",
        "    ground_truths=[None] * len(rtp_prompts),\n",
        "    task='safety',\n",
        "    feature_ids=safety_feature_ids,\n",
        "    alphas=STEERING_ALPHAS,\n",
        "    max_samples=CONFIG.steering_samples\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# HALLUCINATION STEERING (Compare L12 vs L20 vs Multi)\n",
        "# ============================================================================\n",
        "\n",
        "halluc_experiment = run_multilayer_steering_experiment(\n",
        "    prompts=nq_prompts,\n",
        "    ground_truths=nq_ground_truth,\n",
        "    task='hallucination',\n",
        "    feature_ids=halluc_feature_ids,\n",
        "    alphas=STEERING_ALPHAS,\n",
        "    max_samples=CONFIG.steering_samples\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\u2713 ALL MULTI-LAYER STEERING EXPERIMENTS COMPLETE\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcb6a42d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcb6a42d",
        "outputId": "763e2cd7-2d88-474d-91f2-316a78c8d2c7"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ANALYZE MULTI-LAYER STEERING RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "def analyze_multilayer_experiment(experiment: dict, task: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Analyze multi-layer steering experiment results.\n",
        "    Returns DataFrame comparing all modes and \u03b1 values.\n",
        "    \"\"\"\n",
        "    alphas = experiment['alphas']\n",
        "    results_list = []\n",
        "\n",
        "    for mode in ['layer_12', 'layer_20']:\n",
        "        if mode not in experiment['per_mode']:\n",
        "            continue\n",
        "\n",
        "        # Get baseline (\u03b1=0) for this mode\n",
        "        baseline_results = experiment['per_mode'][mode].get(0.0, [])\n",
        "\n",
        "        if task == 'safety':\n",
        "            baseline_mean_tox = np.mean([r['toxicity'] for r in baseline_results]) if baseline_results else 0\n",
        "        else:\n",
        "            baseline_halluc_rate = np.mean([r['hallucinated'] for r in baseline_results]) if baseline_results else 0\n",
        "\n",
        "        for alpha in alphas:\n",
        "            data = experiment['per_mode'][mode].get(alpha, [])\n",
        "            if not data:\n",
        "                continue\n",
        "\n",
        "            ratio_pct = alpha / MEAN_RESIDUAL_NORM * 100 if MEAN_RESIDUAL_NORM > 0 else 0\n",
        "\n",
        "            if task == 'safety':\n",
        "                mean_tox = np.mean([r['toxicity'] for r in data])\n",
        "                std_tox = np.std([r['toxicity'] for r in data])\n",
        "                n_toxic = sum(r['is_toxic'] for r in data)\n",
        "                reduction_pct = (baseline_mean_tox - mean_tox) / baseline_mean_tox * 100 if baseline_mean_tox > 0 else 0\n",
        "\n",
        "                results_list.append({\n",
        "                    'mode': mode,\n",
        "                    'alpha': alpha,\n",
        "                    'ratio_pct': ratio_pct,\n",
        "                    'mean_toxicity': mean_tox,\n",
        "                    'std_toxicity': std_tox,\n",
        "                    'n_toxic': n_toxic,\n",
        "                    'n_samples': len(data),\n",
        "                    'reduction_vs_baseline': reduction_pct\n",
        "                })\n",
        "            else:\n",
        "                halluc_rate = np.mean([r['hallucinated'] for r in data])\n",
        "                n_halluc = sum(r['hallucinated'] for r in data)\n",
        "                reduction_pct = (baseline_halluc_rate - halluc_rate) / baseline_halluc_rate * 100 if baseline_halluc_rate > 0 else 0\n",
        "\n",
        "                results_list.append({\n",
        "                    'mode': mode,\n",
        "                    'alpha': alpha,\n",
        "                    'ratio_pct': ratio_pct,\n",
        "                    'halluc_rate': halluc_rate,\n",
        "                    'n_hallucinated': n_halluc,\n",
        "                    'n_samples': len(data),\n",
        "                    'reduction_vs_baseline': reduction_pct\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(results_list)\n",
        "\n",
        "\n",
        "# Analyze both experiments\n",
        "safety_analysis = analyze_multilayer_experiment(safety_experiment, 'safety')\n",
        "halluc_analysis = analyze_multilayer_experiment(halluc_experiment, 'hallucination')\n",
        "\n",
        "# ============================================================================\n",
        "# PRINT COMPREHENSIVE RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MULTI-LAYER SAFETY STEERING ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n{'Mode':<12} {'\u03b1':>8} {'%\u2016h\u2016':>8} {'Mean Tox':>12} {'N Toxic':>10} {'\u0394 vs \u03b1=0':>12}\")\n",
        "print(\"-\"*80)\n",
        "for _, row in safety_analysis.iterrows():\n",
        "    print(f\"{row['mode']:<12} {row['alpha']:>8.2f} {row['ratio_pct']:>7.0f}% \"\n",
        "          f\"{row['mean_toxicity']:>12.6f} {row['n_toxic']:>10.0f} {row['reduction_vs_baseline']:>+11.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MULTI-LAYER HALLUCINATION STEERING ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n{'Mode':<12} {'\u03b1':>8} {'%\u2016h\u2016':>8} {'Halluc Rate':>14} {'N Halluc':>12} {'\u0394 vs \u03b1=0':>12}\")\n",
        "print(\"-\"*80)\n",
        "for _, row in halluc_analysis.iterrows():\n",
        "    print(f\"{row['mode']:<12} {row['alpha']:>8.2f} {row['ratio_pct']:>7.0f}% \"\n",
        "          f\"{row['halluc_rate']*100:>13.1f}% {row['n_hallucinated']:>12.0f} {row['reduction_vs_baseline']:>+11.1f}%\")\n",
        "\n",
        "# ============================================================================\n",
        "# FIND OPTIMAL CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"OPTIMAL STEERING CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# For each task, find best (mode, \u03b1) combination\n",
        "for task_name, analysis_df in [(\"Safety\", safety_analysis), (\"Hallucination\", halluc_analysis)]:\n",
        "    non_baseline = analysis_df[analysis_df['alpha'] > 0]\n",
        "    if len(non_baseline) > 0:\n",
        "        best_idx = non_baseline['reduction_vs_baseline'].idxmax()\n",
        "        best_row = analysis_df.loc[best_idx]\n",
        "        print(f\"\\n{task_name}:\")\n",
        "        print(f\"  Best Mode: {best_row['mode']}\")\n",
        "        print(f\"  Best \u03b1: {best_row['alpha']:.2f} ({best_row['ratio_pct']:.0f}% of \u2016h\u2016)\")\n",
        "        print(f\"  Reduction vs baseline: {best_row['reduction_vs_baseline']:+.1f}%\")\n",
        "\n",
        "        # Compare modes at their best \u03b1\n",
        "        print(f\"\\n  Mode Comparison (at best \u03b1 for each):\")\n",
        "        for mode in ['layer_12', 'layer_20']:\n",
        "            mode_data = non_baseline[non_baseline['mode'] == mode]\n",
        "            if len(mode_data) > 0:\n",
        "                best_mode_idx = mode_data['reduction_vs_baseline'].idxmax()\n",
        "                best_mode_row = mode_data.loc[best_mode_idx]\n",
        "                print(f\"    {mode}: \u03b1={best_mode_row['alpha']:.2f} \u2192 {best_mode_row['reduction_vs_baseline']:+.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5d2fa8a",
      "metadata": {
        "id": "e5d2fa8a"
      },
      "source": [
        "## \ud83d\udcc8 Visualizations\n",
        "\n",
        "Generate plots showing the impact of steering on toxicity and output quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d377d96e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "d377d96e",
        "outputId": "8220dd8a-95a0-4297-91c1-8b9d6f7f6426"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# COMPREHENSIVE MULTI-LAYER VISUALIZATIONS\n",
        "# ============================================================================\n",
        "\n",
        "fig = plt.figure(figsize=(20, 16))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
        "\n",
        "# Color scheme for modes\n",
        "mode_colors = {'layer_12': '#3498db', 'layer_20': '#e74c3c', }\n",
        "mode_labels = {'layer_12': 'Layer 12', 'layer_20': 'Layer 20', }\n",
        "\n",
        "# ============================================================================\n",
        "# 1. DETECTION ROC-AUC COMPARISON BY LAYER\n",
        "# ============================================================================\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "\n",
        "tasks = ['Hallucination', 'Safety']\n",
        "x = np.arange(len(tasks))\n",
        "width = 0.25\n",
        "\n",
        "for i, layer_key in enumerate(['layer_12', 'layer_20', 'multi_layer']):\n",
        "    aucs = []\n",
        "    for detectors in [halluc_detectors, safety_detectors]:\n",
        "        if layer_key in detectors:\n",
        "            aucs.append(detectors[layer_key]['metrics']['auroc'])\n",
        "        else:\n",
        "            aucs.append(0)\n",
        "    ax1.bar(x + (i - 1) * width, aucs, width, label=mode_labels.get(layer_key.replace('_layer', ''), layer_key),\n",
        "            color=mode_colors.get(layer_key.replace('_layer', ''), '#95a5a6'), edgecolor='black')\n",
        "\n",
        "ax1.axhline(0.5, linestyle='--', color='red', alpha=0.5, label='Random (0.5)')\n",
        "ax1.set_ylabel('ROC-AUC', fontsize=12)\n",
        "ax1.set_title('Detection Performance by Layer', fontsize=14, fontweight='bold')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(tasks)\n",
        "ax1.set_ylim(0, 1)\n",
        "ax1.legend()\n",
        "\n",
        "# ============================================================================\n",
        "# 2. HALLUCINATION RATE BY MODE AND \u03b1\n",
        "# ============================================================================\n",
        "ax2 = fig.add_subplot(gs[0, 1:3])\n",
        "\n",
        "for mode in ['layer_12', 'layer_20']:\n",
        "    mode_data = halluc_analysis[halluc_analysis['mode'] == mode]\n",
        "    if len(mode_data) > 0:\n",
        "        ax2.plot(mode_data['alpha'], mode_data['halluc_rate'] * 100, 'o-',\n",
        "                 color=mode_colors[mode], label=mode_labels[mode], markersize=10, linewidth=2)\n",
        "\n",
        "ax2.set_xlabel('\u03b1 (steering strength)', fontsize=12)\n",
        "ax2.set_ylabel('Hallucination Rate (%)', fontsize=12)\n",
        "ax2.set_title('Hallucination Rate by Steering Mode & Strength', fontsize=14, fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.set_ylim(0, 100)\n",
        "ax2.axhline(halluc_analysis[halluc_analysis['alpha']==0]['halluc_rate'].values[0]*100,\n",
        "            color='gray', linestyle='--', alpha=0.5, label='Baseline')\n",
        "\n",
        "# ============================================================================\n",
        "# 3. TOXICITY BY MODE AND \u03b1\n",
        "# ============================================================================\n",
        "ax3 = fig.add_subplot(gs[1, 0])\n",
        "\n",
        "for mode in ['layer_12', 'layer_20']:\n",
        "    mode_data = safety_analysis[safety_analysis['mode'] == mode]\n",
        "    if len(mode_data) > 0:\n",
        "        ax3.plot(mode_data['alpha'], mode_data['mean_toxicity'], 'o-',\n",
        "                 color=mode_colors[mode], label=mode_labels[mode], markersize=8, linewidth=2)\n",
        "\n",
        "ax3.set_xlabel('\u03b1', fontsize=12)\n",
        "ax3.set_ylabel('Mean Toxicity', fontsize=12)\n",
        "ax3.set_title('Toxicity by Mode', fontsize=12, fontweight='bold')\n",
        "ax3.legend()\n",
        "ax3.set_ylim(bottom=0)\n",
        "\n",
        "# ============================================================================\n",
        "# 4. REDUCTION % COMPARISON\n",
        "# ============================================================================\n",
        "ax4 = fig.add_subplot(gs[1, 1])\n",
        "\n",
        "# Get best reduction for each mode (at any non-zero \u03b1)\n",
        "best_reductions = {'halluc': {}, 'safety': {}}\n",
        "for mode in ['layer_12', 'layer_20']:\n",
        "    h_data = halluc_analysis[(halluc_analysis['mode'] == mode) & (halluc_analysis['alpha'] > 0)]\n",
        "    s_data = safety_analysis[(safety_analysis['mode'] == mode) & (safety_analysis['alpha'] > 0)]\n",
        "    if len(h_data) > 0:\n",
        "        best_reductions['halluc'][mode] = h_data['reduction_vs_baseline'].max()\n",
        "    if len(s_data) > 0:\n",
        "        best_reductions['safety'][mode] = s_data['reduction_vs_baseline'].max()\n",
        "\n",
        "modes = ['layer_12', 'layer_20']\n",
        "x = np.arange(len(modes))\n",
        "width = 0.35\n",
        "\n",
        "h_vals = [best_reductions['halluc'].get(m, 0) for m in modes]\n",
        "s_vals = [best_reductions['safety'].get(m, 0) for m in modes]\n",
        "\n",
        "ax4.bar(x - width/2, h_vals, width, label='Hallucination', color='#3498db', edgecolor='black')\n",
        "ax4.bar(x + width/2, s_vals, width, label='Toxicity', color='#9b59b6', edgecolor='black')\n",
        "\n",
        "ax4.axhline(0, color='black', linewidth=0.5)\n",
        "ax4.set_xlabel('Steering Mode', fontsize=12)\n",
        "ax4.set_ylabel('Best Reduction (%)', fontsize=12)\n",
        "ax4.set_title('Best Reduction by Mode', fontsize=12, fontweight='bold')\n",
        "ax4.set_xticks(x)\n",
        "ax4.set_xticklabels([mode_labels[m] for m in modes])\n",
        "ax4.legend()\n",
        "\n",
        "# ============================================================================\n",
        "# 5. DETECTION METRICS BY LAYER\n",
        "# ============================================================================\n",
        "ax5 = fig.add_subplot(gs[1, 2])\n",
        "\n",
        "metrics = ['Accuracy', 'F1', 'AUC']\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.25\n",
        "\n",
        "for i, layer_key in enumerate(['layer_12', 'layer_20', 'multi_layer']):\n",
        "    if layer_key in halluc_detectors:\n",
        "        m = halluc_detectors[layer_key]['metrics']\n",
        "        vals = [m['accuracy'], m['f1'], m['auroc']]\n",
        "        ax5.bar(x + (i - 1) * width, vals, width,\n",
        "                label=mode_labels.get(layer_key.replace('_layer', ''), layer_key),\n",
        "                color=mode_colors.get(layer_key.replace('_layer', ''), '#95a5a6'), edgecolor='black')\n",
        "\n",
        "ax5.set_xticks(x)\n",
        "ax5.set_xticklabels(metrics)\n",
        "ax5.set_ylabel('Score', fontsize=12)\n",
        "ax5.set_title('Hallucination Detection Metrics', fontsize=12, fontweight='bold')\n",
        "ax5.legend()\n",
        "ax5.set_ylim(0, 1.1)\n",
        "\n",
        "# ============================================================================\n",
        "# 6. FEATURE CORRELATION COMPARISON\n",
        "# ============================================================================\n",
        "ax6 = fig.add_subplot(gs[2, 0])\n",
        "\n",
        "if 'layer_12' in halluc_features and 'layer_20' in halluc_features:\n",
        "    l12_corrs = halluc_features['layer_12']['all_features']['raw_corr'].values[:100]\n",
        "    l20_corrs = halluc_features['layer_20']['all_features']['raw_corr'].values[:100]\n",
        "\n",
        "    ax6.hist(l12_corrs, bins=30, alpha=0.6, color=mode_colors['layer_12'], label='Layer 12', edgecolor='black')\n",
        "    ax6.hist(l20_corrs, bins=30, alpha=0.6, color=mode_colors['layer_20'], label='Layer 20', edgecolor='black')\n",
        "    ax6.axvline(0, color='black', linestyle='--', linewidth=2)\n",
        "    ax6.set_xlabel('Correlation with Hallucination', fontsize=12)\n",
        "    ax6.set_ylabel('Feature Count', fontsize=12)\n",
        "    ax6.set_title('Top 100 Feature Correlations', fontsize=12, fontweight='bold')\n",
        "    ax6.legend()\n",
        "\n",
        "# ============================================================================\n",
        "# 7. TOP FEATURES BY LAYER\n",
        "# ============================================================================\n",
        "ax7 = fig.add_subplot(gs[2, 1])\n",
        "\n",
        "if 'layer_12' in halluc_features:\n",
        "    top_l12 = halluc_features['layer_12']['all_features'].head(10)\n",
        "    y_pos = np.arange(len(top_l12))\n",
        "    colors = ['#e74c3c' if c > 0 else '#27ae60' for c in top_l12['raw_corr']]\n",
        "\n",
        "    ax7.barh(y_pos, top_l12['raw_corr'].values, color=colors, edgecolor='black', alpha=0.8)\n",
        "    ax7.set_yticks(y_pos)\n",
        "    ax7.set_yticklabels([f\"L12-F{int(f)}\" for f in top_l12['feature_id']])\n",
        "    ax7.set_xlabel('Correlation', fontsize=12)\n",
        "    ax7.set_title('Top 10 Layer 12 Features', fontsize=12, fontweight='bold')\n",
        "    ax7.axvline(0, color='black', linewidth=0.5)\n",
        "    ax7.invert_yaxis()\n",
        "\n",
        "# ============================================================================\n",
        "# 8. COMPREHENSIVE SUMMARY\n",
        "# ============================================================================\n",
        "ax8 = fig.add_subplot(gs[2, 2])\n",
        "ax8.axis('off')\n",
        "\n",
        "# Get best configurations\n",
        "best_h = halluc_analysis[halluc_analysis['alpha'] > 0].loc[halluc_analysis[halluc_analysis['alpha'] > 0]['reduction_vs_baseline'].idxmax()] if len(halluc_analysis[halluc_analysis['alpha'] > 0]) > 0 else None\n",
        "best_s = safety_analysis[safety_analysis['alpha'] > 0].loc[safety_analysis[safety_analysis['alpha'] > 0]['reduction_vs_baseline'].idxmax()] if len(safety_analysis[safety_analysis['alpha'] > 0]) > 0 else None\n",
        "\n",
        "summary_text = f\"\"\"\n",
        "MULTI-LAYER EXPERIMENT SUMMARY\n",
        "==============================\n",
        "\n",
        "Layers Compared: 12, 20, Multi(12+20)\n",
        "\n",
        "Dataset Size:\n",
        "  - NQ-Open: {len(nq_df)} samples\n",
        "  - RTP + HH: {len(safety_df)} samples\n",
        "\n",
        "Detection (ROC-AUC):\n",
        "  Layer 12:  {halluc_detectors.get('layer_12', {}).get('metrics', {}).get('auroc', 'N/A'):.3f} (Halluc)\n",
        "  Layer 20:  {halluc_detectors.get('layer_20', {}).get('metrics', {}).get('auroc', 'N/A'):.3f} (Halluc)\n",
        "  Multi:     {halluc_detectors.get('multi_layer', {}).get('metrics', {}).get('auroc', 'N/A'):.3f} (Halluc)\n",
        "\n",
        "Best Steering Config:\n",
        "  Hallucination: {best_h['mode'] if best_h is not None else 'N/A'}\n",
        "    \u03b1={best_h['alpha']:.2f if best_h is not None else 0}, {best_h['reduction_vs_baseline']:+.1f}% if best_h is not None else 'N/A'\n",
        "\n",
        "  Toxicity: {best_s['mode'] if best_s is not None else 'N/A'}\n",
        "    \u03b1={best_s['alpha']:.2f if best_s is not None else 0}, {best_s['reduction_vs_baseline']:+.1f}% if best_s is not None else 'N/A'\n",
        "\"\"\"\n",
        "\n",
        "ax8.text(0.05, 0.95, summary_text, transform=ax8.transAxes, fontsize=10,\n",
        "         verticalalignment='top', fontfamily='monospace',\n",
        "         bbox=dict(boxstyle='round', facecolor='#f0f0f0', edgecolor='#cccccc', alpha=0.9))\n",
        "\n",
        "plt.suptitle('Multi-Layer SAE Analysis: Layer 12 vs Layer 20 vs Combined', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.savefig('multilayer_results.png', dpi=200, bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "print(\"\\n\u2713 Saved visualization to multilayer_results.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fff01c3b",
      "metadata": {
        "id": "fff01c3b"
      },
      "source": [
        "## \ud83d\udcac Example Outputs\n",
        "\n",
        "Let's look at some specific examples to see how steering affects the outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5308ead3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5308ead3",
        "outputId": "554fcb46-f715-4233-8c60-d2a1c682e333"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SHOW EXAMPLE OUTPUTS: COMPARE STEERING MODES\n",
        "# ============================================================================\n",
        "\n",
        "# Find the best \u03b1 for each mode\n",
        "best_configs = {}\n",
        "for mode in ['layer_12', 'layer_20']:\n",
        "    mode_data = halluc_analysis[(halluc_analysis['mode'] == mode) & (halluc_analysis['alpha'] > 0)]\n",
        "    if len(mode_data) > 0:\n",
        "        best_idx = mode_data['reduction_vs_baseline'].idxmax()\n",
        "        best_alpha = mode_data.loc[best_idx, 'alpha']\n",
        "        best_configs[mode] = best_alpha\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXAMPLE OUTPUTS: BASELINE vs BEST STEERING PER MODE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get baseline results (any mode at \u03b1=0 is the same)\n",
        "baseline = halluc_experiment['per_mode']['layer_12'].get(0.0, [])\n",
        "\n",
        "for i in range(min(3, len(baseline))):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"[Example {i+1}]\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    base = baseline[i]\n",
        "    print(f\"Question: {base['prompt'][:80]}...\")\n",
        "    gt = base.get('ground_truth', [])\n",
        "    print(f\"Ground truth: {gt[:2] if gt else 'N/A'}...\")\n",
        "\n",
        "    base_status = \"HALLUC\" if base['hallucinated'] else \"OK\"\n",
        "    print(f\"\\n  \ud83d\udd34 Baseline (\u03b1=0) [{base_status}]:\")\n",
        "    print(f\"     {base['text'][:150]}...\")\n",
        "\n",
        "    for mode, best_alpha in best_configs.items():\n",
        "        steered = halluc_experiment['per_mode'][mode].get(best_alpha, [])\n",
        "        if i < len(steered):\n",
        "            steer = steered[i]\n",
        "            steer_status = \"HALLUC\" if steer['hallucinated'] else \"OK\"\n",
        "            emoji = {'layer_12': '\ud83d\udd35', 'layer_20': '\ud83d\udd34'}[mode]\n",
        "            print(f\"\\n  {emoji} {mode.upper()} (\u03b1={best_alpha:.2f}) [{steer_status}]:\")\n",
        "            print(f\"     {steer['text'][:150]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAFETY EXAMPLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "baseline_safety = safety_experiment['per_mode']['layer_12'].get(0.0, [])\n",
        "\n",
        "for i in range(min(3, len(baseline_safety))):\n",
        "    print(f\"\\n[Safety Example {i+1}]\")\n",
        "    base = baseline_safety[i]\n",
        "    print(f\"Prompt: {base['prompt'][:60]}...\")\n",
        "    print(f\"  Baseline toxicity: {base['toxicity']:.4f}\")\n",
        "\n",
        "    for mode in ['layer_12', 'layer_20']:\n",
        "        mode_data = safety_analysis[(safety_analysis['mode'] == mode) & (safety_analysis['alpha'] > 0)]\n",
        "        if len(mode_data) > 0:\n",
        "            best_alpha = mode_data.loc[mode_data['reduction_vs_baseline'].idxmax(), 'alpha']\n",
        "            steered = safety_experiment['per_mode'][mode].get(best_alpha, [])\n",
        "            if i < len(steered):\n",
        "                print(f\"  {mode} (\u03b1={best_alpha:.2f}): toxicity={steered[i]['toxicity']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df45da10",
      "metadata": {
        "id": "df45da10"
      },
      "source": [
        "---\n",
        "\n",
        "# \u2705 Multi-Layer Experiment Complete!\n",
        "\n",
        "## Summary of Multi-Layer SAE Approach\n",
        "\n",
        "### Motivation (TA Zihao's Suggestion)\n",
        "> \"Hook SAE to multiple layers of the LLM to potentially boost performance\"\n",
        "\n",
        "### Implementation\n",
        "1. **Layer Selection**: Layers 12 (mid-layer, structural) and 20 (late-layer, semantic)\n",
        "2. **SAE Loading**: Separate 16k-feature SAEs from Gemma Scope for each layer\n",
        "3. **Feature Concatenation**: 32k total features when combining both layers\n",
        "4. **Comparison**: L12-only vs L20-only vs Multi-layer\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "#### Detection Performance\n",
        "| Layer Config | Features | Hallucination AUC | Safety AUC |\n",
        "|-------------|----------|-------------------|------------|\n",
        "| Layer 12    | 16k      | See results above | See above  |\n",
        "| Layer 20    | 16k      | See results above | See above  |\n",
        "| Multi-layer | 32k      | See results above | See above  |\n",
        "\n",
        "#### Steering Effectiveness\n",
        "- Compared steering at Layer 12 only, Layer 20 only, and both simultaneously\n",
        "- Multi-layer steering applies vectors at both layers during generation\n",
        "- Results show which layer(s) are most effective for reducing harmful outputs\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "**Layer 12 (Mid-layer)**:\n",
        "- Captures more structural/syntactic patterns\n",
        "- May be better for detecting surface-level indicators\n",
        "\n",
        "**Layer 20 (Late-layer)**:\n",
        "- Captures more semantic/contextual patterns\n",
        "- May be better for understanding meaning and intent\n",
        "\n",
        "**Multi-layer Combination**:\n",
        "- Combines information from both levels of abstraction\n",
        "- Potentially more robust detection\n",
        "- Steering at both layers may have complementary effects\n",
        "\n",
        "### Files Generated\n",
        "- `multilayer_results.png`: Comprehensive 8-panel comparison visualization\n",
        "\n",
        "### Comparison to Single-Layer Baseline\n",
        "- Original implementation used only layer 12\n",
        "- This notebook compares against layer 20 and combined approach\n",
        "- Demonstrates whether multi-layer provides improvement\n",
        "\n",
        "### Future Work\n",
        "- Try additional layer combinations (e.g., layers 8, 16, 24)\n",
        "- Weight optimization for multi-layer feature importance\n",
        "- Layer-specific steering strength tuning\n",
        "- Analyze which features are unique to each layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "__bHYsrImSsB",
      "metadata": {
        "id": "__bHYsrImSsB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}